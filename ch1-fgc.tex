Addressing the final perspective, this thesis will discuss our work in Fine-Grained cryptography. With my coauthors Lincoln and Vassilevska Williams, we built the first fine-grained key exchange built from fine-grained assumptions.

Modern cryptography has developed a variety of important cryptographic primitives, from One-Way Functions (OWFs) to Public-Key Cryptography to Obfuscation. 
Except for a few more limited information theoretic results \cite{Shamir79,CKGS98,RW02}, cryptography has so far required making a computational assumption, P $\neq$ NP being a baseline requirement.
Barring unprecedented progress in computational complexity, such hardness hypotheses seem necessary in order to obtain most useful primitives. 
To alleviate this reliance on unproven assumptions, it is good to build cryptography from a variety of extremely different, believable assumptions: if a technique disproves one hypothesis, the unrelated ones might still hold. Due to this, there are many different cryptographic assumptions: on factoring, discrete logarithm, shortest vector in lattices and many more.

%** - Discrete logarithm is an optioon

% (Secret-sharing, PIR, entropic security, quantum??)


Unfortunately, almost all hardness assumptions used so far have the same quite stringent requirements: not only that NP is not in BPP, but that we must be able to efficiently sample polynomially-hard instances whose solution we know. Impagliazzo \cite{Impagliazzo5worlds,RR94} defined five worlds, which capture the state of cryptography, depending on which assumptions happen to fail. The three worlds worst for cryptography are Algorithmica (NP in BPP), Heuristica (NP is not in BPP but NP problems are easy on average) and Pessiland (there are NP problems that are hard on average but solved hard instances are hard to sample, and OWFs do not exist). 
This brings us to our main question.

\begin{center}
 \emph{	Can we have a meaningful notion of cryptography even if we live in Pessiland (or Algorithmica or Heuristica)?}
\end{center}

This question motivates a weaker notion of cryptography: cryptography that is secure against $n^k$-time bounded adversaries, for a constant $k$. Let us see why such cryptography might exist even if P $=$ NP. In complexity, for most interesting computational models, we have time hierarchy theorems that say that there are problems solvable in $O(n^2)$ time (say) that cannot be solved in $O(n^{2-\epsilon})$ time for any $\epsilon>0$ \cite{HS65,HS66,Tse56}. In fact, such theorems exist also for the average case time complexity of problems \cite{Lev73}. Thus, even if P$=$NP, there are problems that are hard on average for specific runtimes, i.e. {\em fine-grained} hard on average. {\em Can we use such hard problems to build useful cryptographic primitives?} 

Unfortunately, the problems from the time hierarchy theorems are difficult to work with, a common problem in the search for unconditional results. Thus, let us relax our requirements and consider hardness assumptions, but this time on the exact running time of our problems of interest. One simple approach is to consider all known constructions of Public Key Cryptography (PKC) to date and see what they imply if the hardness of the underlying problem is relaxed to be $n^{k-o(1)}$ for a fixed $k$ (as it would be in Pessiland). Some of the known schemes are extremely efficient. For instance, the RSA and Diffie-Hellman cryptosystems immediately imply weak PKC if one changes their assumptions to be about polynomial hardness \cite{rsa,DiffieHellman}. However, these cryptosystems have other weaknesses -- for instance, they are completely broken in a postquantum world as Shor's algorithm breaks their assumptions in essentially quadratic time \cite{Shor}. Thus, it makes sense to look at the cryptosystems based on other assumptions. Unfortunately, largely because cryptography has mostly focused on the gap between polynomial and superpolynomial time, most reductions building PKC have a significant (though polynomial) overhead; many require, for example, multiple rounds of Gaussian elimination. As a simple example,
the Goldreich-Levin  construction for hard-core bits uses $n^{\omega}$ (where $\omega\in [2,2.373)$ is the exponent of square matrix multiplication \cite{VVWmmfaster}\cite{legallMM}) time and $n$ calls to the hard-core-bit distinguisher \cite{hardCoreBitsAndXorLemmaFromGL}. The polynomial overhead of such reductions means that if the relevant problem is only $n^{2-o(1)}$ hard, instead of super-polynomially hard, the reduction will not work anymore and won't produce a meaningful cryptographic primitive. Moreover, reductions with fixed polynomial overheads are no longer composable in the same way when we consider weaker, polynomial gap cryptography. Thus, new, more careful cryptographic reductions are needed.

Ball et al.~\cite{avgCaseFineGrained,eprintAvgCaseFG} recently began to address this issue through the lens of the recently blossoming field of {\em fine-grained complexity}.
%The main idea is as follows:
%While the problems from the time hierarchy theorems seem difficult to work with, there are several very simple, structured problems that are conjectured to require $n^{2-o(1)}$ time to solve on average, say on a RAM. These problems come from a recently blossoming field: fine-grained complexity, 
Fine-grained complexity is built upon ``fine-grained'' hypotheses on the (worst-case) hardness of a small number of key problems. Each of these key problems $K$, has a simple algorithm using a combination of textbook techniques, running in time $T(n)$ on instances of size $n$, in, say, the RAM model of computation. However, despite decades of research, no $\~O(T(n)^{1-\epsilon})$ algorithm is known for any $\epsilon>0$ (note that the tilde $~$ suppresses sub-polynomial factors). The fine-grained hypothesis for $K$ is then that $K$ requires $T(n)^{1-o(1)}$ time in the RAM model of computation. Some of the main hypotheses in fine-grained complexity (see \cite{icm-survey}) set $K$ to be CNF-SAT (with $T(n)=2^n$, where $n$ is the number of variables), or the \kSum~problem (with $T(n)=n^{\lceil k/2\rceil}$), or the All-Pairs Shortest Paths problem (with $T(n)=n^3$ where $n$ is the number of vertices), or one of several versions of the $k$-Clique problem in weighted graphs.
Fine-grained uses fine-grained reductions between problems in a very tight way (see \cite{icm-survey}): if problem $A$ has requires running time $a(n)^{1-o(1)}$, and one obtains an $(a(n),b(n))$-fine-grained reduction from $A$ to $B$, then problem $B$ needs runtime $b(n)^{1-o(1)}$. Using such reductions, one can obtain strong lower bounds for many problems, conditioned on one of the few key hypotheses.


%While fine-grained complexity is typically about worst-case hardness assumptions, several of its key hard problems are conjectured to also be hard on average: e.g. $k$-SUM where the range of the integers is roughly between $-n^k$ and $n^k$. 

The main question that Ball et al. set out to answer is: {\em Can one use fine-grained reductions from the hard problems from fine-grained complexity to build useful cryptographic primitives?} Their work produced worst-case to average-case fine-grained reductions from key problems to new algebraic average case problems. %to a new problem %that can be solved in roughly the same time for each of several key problems from fine-grained complexity. 
%This gave, for example, a problem that requires $n^{2-o(1)}$ time to solve on average, based on the \ThSum~conjecture. 
From these new problems, Ball et al. were able to construct fine-grained proofs of work, but they were not able to obtain stronger cryptographic primitives such as fine-grained one-way-functions or public key encryption. In fact, they gave a barrier for their approach: extending their approach would falsify the Nondeterministic Strong Exponential Time Hypothesis (NSETH) of Carmosino et al. \cite{CarmosinoGIMPS16}. Because of this barrier, one would either need to develop brand new techniques, or use a different hardness assumption.

\begin{center}{\em What kind of hardness assumptions can be used to obtain public-key cryptography (PKC) even in Pessiland?}\end{center} 

A great type of theorem to address this would be: for every problem $P$ that requires $n^{k-o(1)}$ time on average, one can construct a public-key exchange (say), for which Alice and Bob can exchange a $\lg(n)$ bit key in time $O(n^{ak})$, whereas Eve must take $n^{(a+g)k-o(1)}$ time to learn Alice and Bob's key, where $g$ is large, and $a$ is small. As a byproduct of such a theorem, one can obtain not just OWFs, but even PKC in Pessiland under fine-grained assumptions via the results of Ball et al. Of course, due to the limitations given by Ball et al. such an ideal theorem would have to refute NSETH, and hence would be at the very least difficult to prove.
Thus, let us relax our goal, and ask 
%\begin{center}{\em What properties does a fine-grained average-case assumption need to have so that fine-grained PKC can be built from it?}\end{center} 
\begin{center}{\em What properties are sufficient for a fine-grained average-case assumption so that it implies fine-grained PKC?}\end{center} 

If we could at least resolve this question, then we could focus our search for worst-case to average-case reductions in a useful way.