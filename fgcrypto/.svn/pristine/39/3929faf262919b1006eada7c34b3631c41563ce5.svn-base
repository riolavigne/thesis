\section{Properties of $k$-SUM and Zero $k$-clique Hypotheses}

In this section, we will prove the properties that $k$-SUM and Zero $k$-Clique have that will make them useful in constructing fine-grained OWFs and our fine-grained key exchange.

\subsection{$k$-SUM and Zero $k$-Clique are Plantable from Weak Hypotheses}
Here we will show that by assuming the Weak $k$-SUM hypothesis (see definition \ref{def:weak-k-sum}), we get that $k$-SUM is plantable and $n^{2+\delta}$-\ACIH. The proof is relatively straightforward: just show that planting a solution in a random $k$-SUM-$R$ instance is easy while making sure that the distributions are close to what you expect. The full proof of this is in appendix \ref{sec:zkcAppendix}

\begin{theorem}
	Assuming the weak $k$-SUM-$R$ hypothesis, $k$-SUM-$R$ is plantable with error $\leq 2n^k/R$ in $O(n)$ time.
	%If the weak $k$-sum hypothesis holds over a range $R > 100 n^k$, then $k$-SUM-$R$ is $n^{2+\delta}$-\ACIH.
	\label{thm:ksumPlantable}
\end{theorem}
\begin{proof}
	First, we will define $\Generate(n,b)$:
	\begin{itemize}
		\item $b = 0$: choose all $kn$ entries uniformly at random from $[0,R-1]$, taking time $O(n)$.
		\item $b = 1$: choose all $kn$ entries uniformly at random from $[0,R-1]$, then choose values $v_1, \dots, v_k$, each $v_i$ at random from partition $P_i$, and choose $i \getsr [k]$. Set $v_i = - \sum_{j \neq i} v_j \mod R$. This takes time $O(n)$.
	\end{itemize}
	
	We need to show that $\Generate(n,0)$ is $\epsilon$-close to $D_0$ and $\Generate(n,1)$ is $\epsilon$-close to $D_1$.
	
	First, we note that $\Generate(n,0)$ has the following property: $\Pr_{I \sim \Generate(n,0)}[I = I' | val(I) = 0 ] = \Pr_{I \sim D_0}[I = I']$. This is because $\Generate(n,0)$ samples uniformly over the support of $D_0$. So, the total variation distance between $\Generate(n,0)$ and $D_0$ is the probability $\Generate(n,0)$ samples outside of the support of $D_0$, that is, the probability $\Generate(n,0)$ samples an $I$ with a value $1$ or greater. Now, a union bound gives us
	\[ TVD(\Generate(n,0), D_0) = \Pr_{I \sim \Generate(n,0)}[val(I) \ge 1] \le \sum_{\mbox{all $n^k$ sums $\vec s \in [n]^k$}} \Pr_{I \sim \Generate(n,0)}[\mbox{$\vec s$ is a $k$-sum}] = \frac{n^k}{R}. \]
	
	Now, to show that $\Generate(n,1)$ is $\epsilon$-close to $D_1$, we will use the fact that total-variation distance ($TVD$) is a metric and the triangle inequality. Let $\Generate(n,0) +$ Plant and $D_0 + $ Plant denote sampling from the first distribution and planting a $k$-sum solution at random (so $\Generate(n,0) +$ Plant $= \Generate(n,1)$. We have that \[TVD(\Generate(n,1), D_1) \le TVD(\Generate(n,0) + \mbox{Plant}, D_0 + \mbox{Plant}) + TVD(D_0 + \mbox{Plant}, D_1).\]
	The distance $\Generate(n,0) +$ Plant from $D_0 + $ Plant is equal to the distance from $\Generate(n,0)$ and $D_0$, since the planting does equivalent things. As previously shown, this distance is at most $\frac{n^k}{R}$. The distance from $D_0 +$ Plant and $D_1$ is just the chance that we introduce more than one clique by planting. We are only changing one value in the $D_0$ instance, $v_i$. There are $n^{k-1} - 1 \le n^{k-1}$ possible sums involving $v_i$, so the chance that we accidentally introduce an unintended $k$-sum solution is at most $\frac{n^{k-1}}{R}$. Therefore,
	\[TVD(\Generate(n,1), D_1) \le \frac{n^k}{R} + \frac{n^{k-1}}{R} < \frac{2n^k}{R}\] \qed
\end{proof}

Note that when $R > 6n^k$  $\Generate(n,1)$ has total variation distance $< 1/3$ from $D_1(k-$SUM$-R,n)$.


\begin{theorem}\label{thm:zkcPlantable}
	Assuming the weak Zero $k$-Clique hypothesis over range $R$, Zero $k$-Clique is plantable with error $\leq 2n^k/R$ in $O(n^2)$ time.
\end{theorem}

The proof of this mirrors of the proof that $k$-SUM-$R$ is plantable and is detailed in appendix \ref{sec:zkcAppendix}. Note that the size of a $k$-Clique instance is $O(n^2)$, and so the fact that this requires $O(n^2)$ time is just that it is linear in the input size. Here we will just list what the $\Generate$ functionality is:
\begin{itemize}
	\item $\Generate(n,0)$ outputs a complete $k$-partite graph with $n$ nodes in each partition, and edge weights drawn uniformly from $\Z_R$. This takes $O(n^2)$ time.
	\item $\Generate(n,1)$ starts with $\Generate(n,0)$, and then plants a clique by choosing a node from each partition, $v_1 \in P_1, \dots, v_k \in P_k$, choosing an $i \neq j \getsr [k]$, and setting the weight $w(v_i, v_j) = -\sum_{(i', j') \neq (i,j)} w(v_{i'}, v_{j'}) \mod R$. This also takes $O(n^2)$ time.
\end{itemize}

Unfortunately for general $k$-SUM, the problem does not lend itself well to being average-case list-hard or splittable, as far as we can see. However, we will show that if we assume that Zero $k$-Clique is \emph{search} hard (strictly weaker assumption than being indistinguishably hard), we can get plantable, list-hard, and splittable properties --- the caveat is that we must assume that Zero $k$-Clique requires $\~\Omega(n^k)$ time to solve (not just super-linear in the input size).


\subsection{Zero k-Clique is Plantable, Average Case List-Hard and, Splittable from Strong Hypothesis}
\label{sec:zkcIsAllTheThings}

Recall that the strong hypothesis is actually a search problem, and unfortunately, there is no average-case to average-case reduction from search to decision for Zero $k$-Clique that we are aware of. The existing worst-case reductions break down because of the need for recursion and the error we allow our adversary\footnote{The recurrence really throws us for a loop, so to speak.}.

Now we will prove that Zero $k$-Clique has the properties we need for our key exchange. 

\subsubsection{Zero $k$-Clique is a Plantable Problem}

The proof of this theorem follows from the proof for theorem \ref{thm:zkcPlantable}, even though it was for the decision version of Plantability.
\begin{theorem}
	Zero $k$-Clique is plantable in time $O(n^2)$ with error $\leq 2n^k/R$ under that Strong Zero-$k$-Clique Hypothesis.
	\label{thm:zkcSearchPlantable}
\end{theorem}
The idea is that $\Generate(n)$ is the same as $\Generate(n,1)$ for the decision version. We proved that $\Generate(n,1)$ has TVD from $D_1$ at most $2n^k/R$, which finishes the proof.


\subsubsection{Zero $k$-clique is Average Case List-Hard}

%Also true for the decision version! If we want the decision version instead 
% We can include the commented out points about D_0
We present the proof that zero $k$-clique is average case list-hard in Section \ref{sec:appendixListHard} in Theorem \ref{thm:zkcSelfReduce}.  We will state the the theorem here as well.

\begin{theorem}
	Given the \strongzkc, Zero $k$-Clique is Average Case List-Hard with list length $\ell(n)$ for any $\ell(n) = n^{\Omega(1)}$.
\end{theorem}

The intuition of the proof is as follows. There is an efficient worst case self-reduction for the zero-k-clique problem. This self-reduction results in $ell'(n)^k$ subproblems of size $n/\ell'(n)$. One can choose $\ell'(n)$ of these instances such that they are generated from non-overlapping parts of the original instance. They will then look uniformly randomly generated. 

Now we will have generated many, $\ell^{'k}(n)$ of these list versions of the $k$-clique problem, where only one of them has the unique solution. We show that the problem is Average Case List-Hard by demonstrating that we can make many independent calls to the algorithm despite correlations between the instances called. Specifically, we only care about the response on one of these instances, so as long as that instance is random then we can solve the original problem. 


\subsubsection{Zero $k$-Clique is Splittable}
Next we show that zero-k-clique is splittable. 

Intuitively we will take the first half of the bits of each edge weight and make an instance from that. Then we take the second half of the bits of each edge weight to make another instance. If the $\binom{k}{2}$ weights on a $k$ clique sum to zero then the first half of all the weights sum to zero up to carries, and the second half of all the weights sum to zero up to carries. We simply guess the carries. We start by proving this for a convenient range and then show we can use a reduction to get more arbitrary ranges. 


\newcommand{\GeneralSplit}{\mathsf{GeneralizedSplit}}

For this we need to generalize the notion of splittable.
\begin{definition}[Generalized Splittable]
	A $T(n)$-\ACIH~problem is generalized splittable with error $\epsilon m$ to the problem $P'$ if there exists a $\PFT{T(n)}$ algorithm $\Split$ such that
	\begin{itemize}
		\item when given an instance of a problem $I \sim D_{0}(P,n)$, $\GeneralSplit(I)$ produces a list of length $m$ of pairs of instances  $\{(I_1^1,I_2^1),\ldots,(I_1^m,I_2^m)\}$ where  $\forall i\in [1,m]$ $I_1^i,I_2^i$ are drawn from a distribution with total variation distance at most $\epsilon$ from $D_{0}(P',n)\times D_0(P',n)$.
		\item when given an instance of a problem $I \sim D_{1}(P,n)$, $\GeneralSplit(I)$ produces a list of length $m$ of pairs of instances  $\{(I_1^1,I_2^1),\ldots,(I_1^m,I_2^m)\}$ where $\exists i\in[1,m]$ such that $I_1^i,I_2^i$ are drawn from a distribution with total variation distance at most $\epsilon$ from $D_{1}(P',n)\times D_1(P',n)$.
	\end{itemize}
\end{definition}

%\begin{lemma}
%	If a problem $P$ is splittable with error $\epsilon$ then $P$ is generalized splittable with error $\epsilon$.
%\end{lemma}
%\begin{proof}
%Use the splitting procedure to generate a list of length $m=1$. The outputs are drawn from the appropriate distributions. 
%\end{proof}

Lemma \ref{lem:zkcSplitConvenientRange} shows zero-$k$ clique to be generalized splittable over a convenient range.
Lemma \ref{lem:zkcSplitConvenientRange} is extended to all large enough ranges in Theorem \ref{thm:zkcsplittable}. 

These proofs are presented in Section \ref{sec:zkcAppendix}. The intuition of this proof relies on the idea that if a small number of numbers sum to zero then the high order bits basically sum to zero and the low order bits basically sum to zero. One must guess the carry, which is why use the generalized splittable definition. 