\section{Proofs for Fine-Grained One-Way Functions}
\label{sec:fgowfAppendix}

\begin{theorem}
	If there exists a Plantable $T(n)$-\ACIH~problem where $G(n)$ is $\PFT{T(n)}$ and $\epsilon < 5/12$, then $T(n)$-FGOWFs exist.
	\label{thm:plantableOWF}
\end{theorem}
\begin{proof}
	Let $P$ be a Plantable $T(n)$-\ACIH~problem where $G(n) = T(n)^{1-\delta}$ for some constant $\delta>0$. So, the (randomized) algorithm $\Plant$ is $\PFT{T(n)}$ and takes an instance $I$ with no solutions and an implicit randomness $r$, outputting an instance $I'$ that has at least one solution --- we write this as $\Plant(I; r)$ when explicitly noting which randomness was used.
	
	Since when using our one-way function, we will be generating input instances with $\Generate$, we want to show that being able to invert $\Plant$ over the distribution from $\Generate$ is as hard, in a fine-grained sense, as solving the \ACIH~problem $P$. Let $\epsilon$ be the upper bound on the total variation distance between $\Generate$ and $D_0(P,n)$, and between $\Plant(D_0(P,n))$ and $D_1(P,n)$, as per definition \ref{def:plantable}. This means that $2\epsilon$ is an upper bound on the total variation distance between $\Plant \circ \Generate$ and $D_1(P,n)$.
	
	Now, let $\cA$ be a $\PFT{T(n)}$ algorithm that inverts $\Plant$ on inputs generated by $\Generate$ with probability $\gamma > \frac{1}{6(1 - 2\epsilon)}$. Since $\epsilon < 5/12$, $\frac{1}{6(1 - 2\epsilon)}$ is a constant greater than $0$, and therefore so is $\gamma$. We will show that this violates the assumption that $P$ is a $T(n)$-\ACIH~problem.
	
	We now construct a $\PFT{T(n)}$ algorithm $\cB$ that distinguishes between $I \sim D_0(P,n)$ and $I \sim D_1(P,n)$ with probability greater than $2/3$, solving $P$.
	\begin{itemize}
		\item Given $I$ from distribution $D$, $\cB$ gives $I$ to $\cA$.
		\item $\cA$ outputs $\hat I, r$.
		\item If $\Plant(\hat I; r) == I$, output $1$. Otherwise, output $0$.
	\end{itemize}
	
	First, note that if $I \sim D_0(P,n)$, then there cannot exist any randomness $r$ or instance $I$ such that $\Plant(\hat I; r)$ because $\Plant(\hat I; r)$ always introduces a solution, and is therefore never in $S_0$. So, $\Pr_{I \sim D}[\cB(I) = 0 | I \sim D_0(P,n)] = 1$.
	
	Now, we compute the probability that $\cB$ outputs $1$ when $I' \sim D_1(P,n)$. Let 
	
	\begin{align*}
	p_{generated} &=\Pr_{I \sim D_1(P,n)}[\Plant(\cA(I)) = I | I \sim \Plant \circ \Generate]\\
	p_{generatable} &=\Pr_{I \sim D_1(P,n)}[I \sim \Plant \circ \Generate] 
	\end{align*}
	
	
	We have that
	\begin{align*}
	\Pr_{I \sim D}[\cB(I) = 1 | I \sim D_1(P,n)] &= \Pr_{I \sim D_1(P,n)}[\Plant(\cA(I)) = I ]\\
	&\ge p_{generated} \cdot p_{generatable}\\
	&> \gamma \cdot (1 - 2\epsilon) \ge \frac{(1 - 2\epsilon)}{6(1 - 2\epsilon)} = \frac 1 6.
	\end{align*}
	
	Therefore, the probability that $\cB$ correctly categorizes instances $I$ from $P$ is
	\[ \Pr_{I \sim D}[\cB(I) = val(I)] = \frac 1 2 \Pr_{I \sim D}[\cB(I) = 0 | I \sim D_0(P,n)] + \frac 1 2 \Pr_{I \sim D}[\cB(I) = 1 | I \sim D_1(P,n)] > \frac 1 2 + \frac 1 6 = \frac 2 3. \]
	%TODO: easy peasy
	So, assuming $P$ is $T(n)$-\ACIH, no adversary has better than a constant chance less than 1 of being able to invert $\Plant(I; r)$ is a medium $T(n)$-FGOWF. By claim \ref{thm:medium-strong-owf}, this implies there exist strong $T(n)$-FGOWFs.
\end{proof}



\begin{corollary}
	Assuming either the Weak $k$-SUM hypothesis or weak Zero $k$-Clique hypothesis, there exist FGOWFs with fine-grained hardcore bits.
	\label{cor:hardcorebitkclique}
\end{corollary}
\begin{proof}
	This is straightforward due to the nature of planting for both of these hypotheses. Informally, planting for these problems is choosing a location within the given instance to put a solution. If an adversary learns where that solution is supposed to be, generating an instance without that specific solution is easy.
	
	First, let's prove this for $k$-SUM. The reason $k$-SUM is plantable is because $\Plant$ takes as its randomness $k$ indices in the $k$-SUM instance, and then changes the value one of them to make those $k$ instances form a solution the $k$-SUM. This randomness requires specifying $k$ instances out of $kn$, and an edge-weight. let $y$ be the $k\log(n)$ bits required to describe all $k$ locations that the planted solution lives; $y$ is part of the randomness in this case and we let $r$ denote which of these locations was changed. We can construct the following algorithm that inverts $\Plant(I; y, r)$ when given $y$.
	
	Let $\cB(I' = \Plant(I; y,r), y):$
	\begin{enumerate}
		\item Let $y[1] \in A_1$ denote the entry in list $A_1$ that is part of the $k$-SUM solution.
		\item Let $\hat I$ be $I'$ except with $y[0]$ changed to a different random value in the range $[0, R-1]$.
		\item Output $(\hat I; y, 1)$.
	\end{enumerate}
	
	$\cB$ runs in time $\tilde O(n)$, and is correct. The correctness comes from the fact that we preserve a zero-sum. $\Plant(\hat I; y, 1)$ will output an $I''$ such that $I''$ is just $\hat I$ with one entry changed, $y[1]$. $\hat I$ is the same as $I'$ but also with the same entry changed, $y[1]$. We need to show they are changed to the same value. In both $I''$ and $I'$, $\sum y[i] \equiv 0 \mod R$, and so $y[1] = \sum_{i=2}^k y[i] \mod R$. Because $y[2], \dots, y[k]$ are the same values across $I', \hat I,$ and $I''$, $y[1]$ is the same value in both $I'$ and $I''$. Finally, because we have a sub-polynomially sized advice to give an adversary for a $\PFT{n^k}$ adversary to produce a preimage, the fine-grained GL variant applies to the one-way function $\Plant$.
	
	The proof that Zero $k$-C
	\begin{center}
		
	\end{center}
	lique has this property is analogous: planting chooses a specific location, describable in $\poly(\log(n))$ bits, to plant a clique. Knowing where the clique was planted allows one to quickly produce something in the preimage of a given instance.
\end{proof}.