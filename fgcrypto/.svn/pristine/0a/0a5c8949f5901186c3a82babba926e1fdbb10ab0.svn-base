\section{Fine-Grained One-Way Functions}\label{sec:fg-owfs}

In this section, we give a construction of fine-grained OWFs (FGOWF) based on plantable $T(n)$-\ACIH~problems. We first show that even though the probability of inversion may be constant (called `medium' fine-grained one-way), we can do some standard boosting in the same way weak OWFs can be transformed into strong OWFs in the traditional sense. Then, given such a plantable problem, we will prove that $\Generate(n,1)$ is a medium $T(n)$-FGOWF. Then, from this medium FGOWF, we can compile a strong FGOWF using this boosting trick. Then, since \zkclique~ is plantable (see theorem \ref{thm:zkcPlantable}), this implies \zkclique~ yields fine-grained OWFs.

Finally, we discuss the possibility of fine-grained hardcore bits and pseudorandom generators. It turns out that the standard Goldreich-Levin approach to creating hardcore bits works in a similar fashion here, but requires some finessing; it will not work for \emph{all} fine-grained OWFs.

\subsection{Weak and Strong OWFs in the Fine-Grained Setting}

In traditional cryptography, we have notions of weak and strong OWFs. Weak OWFs can be inverted most of the time, but a polynomial-fraction of the time, they cannot be. These weak OWFs can be compiled into strong OWFs (showing that weak OWFs imply strong OWFs), where there is a negligible chance that the resulting strong OWF is invertable over the choice of inputs.

Here we will briefly discuss how a 'medium' $T(n)$-FGOWF can imply a 'strong' $T(n)$-FGOWF, where 'strong' refers to definition \ref{def:fg-owf}

\begin{definition}
	A function $f$ is a medium $T(n)$-FGOWF if there exists a sub-polynomial function $Q(n)$ such that for all $\PFT{T(n)}$ adversaries $\cA$,
	\[ \Pr_{x \getsr \{0,\}^n}[ \cA(f(x)) \in f\inv(f(x)) ] \le 1 - \frac 1 {Q(n)}. \]
\end{definition}

\begin{claim}\label{thm:medium-strong-owf}
	Medium $T(n)$-FGOWFs imply strong $T(n)$-FGOWFs for any polynomial $T(n)$ that is at least linear.
\end{claim}
\begin{proof}
	The structure of this proof will follow Yao's original argument augmenting weak OWFs to strong ones. Intuitively, we are able to use this argument because sub-polynomial functions compose well with each other.
	
	Let $f$ be a medium $T(n)$-FGOWF, where the probability any $\PFT{T(n)}$ adversary inverts it is $1 - \frac 1 {Q(n)}$. The basic idea will be to produce $f'$ that is just a concatenation of many $f$s, just as in the traditional cryptographic case.
	
	For any constant positive integer function $c(n)$, let $f'(x_1 || \dots || x_{c(n)}) = f(x_1)||$ $\dots ||f(x_{c(n)})$. Let $c(n) = 2Q^2(n)$ and note that $c(n) \cdot n = \~O(n)$, and so $T(c(n) \cdot n) = \~O(T(n))$. $f'$ is a $T(c(n) \cdot n) = O(T(n))$-FGOWF since $c(n)$ is subpolynomial.
	
	Now, for sake of contradiction, let $\cA$ be a $\PFT{T(n)}$ such that there exists a subpolynomial function $p'$ where
	\[\Pr[\cA'(f'(x_1 || \dots || x_c)) \in f'^{-1}(f'(x_1 || \dots || x_c))] \ge \frac 1 {p'(c(n) \cdot n)}.\]
	Let $p(n) = p'(c(n) \cdot n)$. Because $c(n)$ and $p'(c(n))$ are sub-polynomial, $p'(c(n) \cdot n) < O(c(n) \cdot p(n))$, implying $p'(c(n) n)$ is also sub-polynomial. And therefore, there exists a sub-polynomial function $p$ such that
	\[\Pr[\cA'(f'(x_1 || \dots || x_c)) \in f'^{-1}(f'(x_1 || \dots || x_c))] \ge \frac 1 {p(n)}.\]

	We will define a $\PFT{T(n)}$ function $\cA_0$ that makes single call to $\cA$: on input $y = f(x)$
	\begin{enumerate}
		\item Choose $i \getsr [c(n)]$.
		\item Let $y_i = y$
		\item For all $j \in [c(n)]$, $j \neq i$, $x_j \getsr \{0,1\}^n$ and $y_j \getsr f(x_j)$.
		\item Run $\cA$ on $(y_1, \dots, y_{c(n)})$ to get output $(x_1, \dots, x_{c(n)})$ if $\cA$ succeeds.
		\item If $\cA$ succeeded, output $x_i$.
	\end{enumerate}
	
	Because all operations in $\cA_0$ are either calling $\cA$ (once) or take time $O(n\cdot c(n))$,\footnote{It does not make much sense for $T(n)$ to be sublinear for our contexts} $\cA_0$ is a $\PFT{T(n)}$ algorithm. Now, we will let $\cB$ be an algorithm calling $\cA_0$ $d(n) = 4c(n)p(n)Q(n)$ times, returning a valid inversion of $f(x)$ if $\cA$ succeeded at least once.
	
	We will call $x \in \{0,1\}^n$ 'good' if $\cA_0$ inverts it with probability at least $\frac 1 {2c^2(n)p(n)}$; $x$ is 'bad' otherwise. Notice that if $x$ is good, then $\cB$, which runs $\cA$ many times, succeeds with high probability:
	\[ \Pr[\cB(f(x)) \mbox{ fails} | x\mbox{ is good}] \le (1 - \frac 1{2c^2(n)p(n)})^{d(n)} \sim e^{-2Q(n)} < \frac 1 {2Q(n)}. \]
	
	
	We will show that there are at least $2^n(1 - \frac{1}{2p(n)})$ good elements.
	
	
	\begin{claim}
		There are at least $2^n(1 - \frac{1}{2p(n)})$ good elements.
	\end{claim}
	\begin{proof}
		For a contradiction, assume there are at least $2^n(\frac{1}{2p(n)})$ bad elements. We will end up contradicting the inversion probability of $\cA$ (which is at least $1/p(n)$). For notation, let $\vec x = (x_1, \dots, x_{c(n)}) \in \{0,1\}^{n\cdot c(n)}$, and $\vec x$ will be chosen uniformly at random over the input space.
		\begin{align*}
		\Pr_{\vec x}[\cA(\vec y = f'(\vec x)) \mbox{succeeds}] &= \Pr[\cA(\vec y)\mbox{ succeeds} \land \exists \mbox{bad }x_i] + \Pr[\cA(\vec y)\mbox{ succeeds} \land \mbox{ all } x_i \mbox{ good}]
		\end{align*}
		Now, for all $j \in [c(n)]$,
		\begin{align*}
		\Pr_{\vec x}[\cA(\vec y) \mbox{ succeeds} \land x_j\mbox{ is bad}] &\le \Pr_{\vec x}[\cA(\vec y) \mbox{ succeeds} |  x_j\mbox{ is bad}]\\
		%\cdot \Pr_{x_j}[ x_j\mbox{ is bad}]\\
		&\le c(n) \Pr_{\vec x}[\cA_0(f(x_j))\mbox{ succeeds} | x_j\mbox{ is bad}]\\
		&\le \frac{c(n)}{2c^2(n)p(n)} = \frac{1}{2c(n)p(n)}
		\end{align*}
		So, if we just union bound over all $j$, we get
		\[ \Pr_{\vec x}[\cA(\vec y) \mbox{ succeeds} \land \mbox{some } x_i \mbox{ are bad}] \le \sum_{j = 1}^{c(n)} \Pr_{\vec x}[\cA_0(f(x_j))\mbox{ succeeds} \land x_j\mbox{ is bad}] \le \frac{1}{2p(n)}. \]
		And one more quick upper bound yields
		\begin{align*}
		\Pr[\cA(\vec y) \mbox{ succeeds} \land {all } x_i \mbox{ are good}] &\le \Pr_{\vec x}[\mbox{all $x_i$ good}]\\
		&< (1 - \frac{1}{2Q(n)})^{c(n)} \sim e^{-2Q(n)} < \frac{1}{2Q(n)}
		\end{align*}
		Finally, this yields the contradiction that
		\begin{align*}
		\Pr_{\vec x}[\cA(\vec y) \mbox{ succeeds}] < \frac{1}{Q(n)}.
		\end{align*}\qed
	\end{proof}

	Now that we know there is a high probability that we hit a good $x$, we can finish the rest of this proof.
	\begin{align*}
	\Pr_{x}[\cB(f(x)) \mbox{ fails}] &= \Pr[\cB(f(x)) \mbox{ fails} | x \mbox{ is good}] \Pr_x[x \mbox{ is good}] + \Pr[\cB(f(x)) \mbox{ fails} | x \mbox{ is bad}] \Pr_x[x \mbox{ is bad}]\\
	&\le\Pr[\cB(f(x)) \mbox{ fails} | x \mbox{ is good}] \Pr_x[x \mbox{ is good}] + \Pr_x[x \mbox{ is bad}]\\
	&\le \frac{1}{2Q(n)}(1 - \frac 1 {2Q(n)}) + \frac{1}{2Q(n)} < \frac 1 {Q(n)}
	\end{align*}
	Thus, the chance that $\cB$ actually has of inverting $f$ is strictly greater than $1 - \frac 1 {Q(n)}$, contradicting the claim that $f$ was medium-hard with respect to $Q(n)$.\qed
\end{proof}

\paragraph{Weaker Fine-Grained OWFs.} Now, because we are in the fine-grained setting, we can talk about gaps. There is a notion of weak-OWFs in cryptography where we can say if there exists \emph{any} polynomial such that we can invert with probability $1 - 1/\poly$, we can construct strong OWFs. We want a similar notion for fine-grained OWFs. Here we can't just choose any polynomial --- we have to choose a polynomial that respects the gap.

Formally, for an $T(n)$-FGOWF $f$ that has $\PFT{T(n)}$ adversaries inverting it with probability $1 - 1/P(n)$, we can get that a $\PFT{T(n)}$ adversary can invert $f'$ with probability $(1 - 1/P(n))^{c(n)}$. Now, as long as there exists $\delta'$ such that $T(n)^{1-\delta}P(n) = T(n)^{1-\delta'}$, there is still a gap ($\delta' < \delta$) even if we compute $f$ $P(n)$ times to evaluate $f'$. Therefore, we are able to get a strong fine-grained OWF from a weak one, as long as it's not \emph{too} weak.

\subsection{Building Fine-Grained OWFs from Plantable Problems}\label{sec:building-fgowfs}

One can generate fine-grained one way functions from plantable problems.

\begin{theorem}\label{thm:FGOWFs-exist}	%\label{thm:plantableOWF}
	If there exists a Plantable $T(n)$-\ACIH~problem where $G(n)$ is $\PFT{T(n)}$ and $\epsilon < 1/3$, then $T(n)$-FGOWFs exist.
\end{theorem}
\begin{proof}
	Let $P$ be a Plantable $T(n)$-\ACIH~problem where $G(n) = T(n)^{1-\delta}$ for some constant $\delta>0$. So, the (randomized) algorithm $\Generate(n,1)$ is $\PFT{T(n)}$ and outputs an instance $I$ that has at least one solution --- we write this as $\Generate(n,1; r)$ when explicitly noting which randomness was used.
	
	We want to show that being able to invert $\Generate(n,1; r)$, over the distribution from $r$, in a fine-grained sense, as solving the \ACIH~problem $P$. Let $\epsilon$ be the upper bound on the total variation distance between $\Generate(n,1)$ and $D_1(P,n)$, as per definition \ref{def:plantable}.
	
	For, sake of contradiction, first assume that no \emph{medium} $T(n)$-FGOWF exist. So, we can invert $\Generate(n,1)$ with any probability $1 - \frac 1 {Q(n)}$ for any sub-polynomial $Q(n)$. Let $\cA$ be a $\PFT{T(n)}$ algorithm that inverts $\Generate(n,1)$ with probability $\gamma > \frac{1 + 3\epsilon }{3(1 - \epsilon)}$; $\epsilon$ is a constant less than $1/3$, so $\gamma$ is a constant less than $1$. Thus, given our assumption, $\cA$ exists. We will show that this violates the assumption that $P$ is a $T(n)$-\ACIH~problem.
	
	We now construct a $\PFT{T(n)}$ algorithm $\cB$ that distinguishes between $I \sim D_0(P,n)$ and $I \sim D_1(P,n)$ with probability greater than $2/3$, solving $P$.
	\begin{itemize}
		\item Given $I$ from distribution $D$, $\cB$ gives $I$ to $\cA$.
		\item $\cA$ outputs $r$.
		\item If $\Generate(n,1;r) == I$, output $1$. Otherwise, output $0$.
	\end{itemize}
	
	\xxx{Rio: This is not good. We need to rewrite this proof completely with our current definitions. It shouldn't be so complicated, either.}
	
	We will now compute the probability that $\cB$ distinguishes between inputs from $D_1$ and $D_0$. Recall that $D$ is just sampling with $D_0$ with probability $1/2$, and otherwise samples from $D_1$. We have  \[\Pr_{I \sim D}[\cB(I) \mbox{ distinguishes }] = \Pr_{I \sim D}[\cB(I) = 1 | I \sim D_1] \cdot \Pr_{I \sim D}[I \sim D_1] + \Pr_{I \sim D}[\cB(I) = 0 | I \sim D_0] \cdot \Pr_{I \sim D}[I \sim D_0]. \]
	Unpacking this, we see that $\Pr_{I \sim D}[\cB(I) = 1 | I \sim D_1] = \Pr_{I \sim D}[\cB(I) = 1 | I \sim D_1 \land I \sim \Generate(n,1)] \cdot \Pr[I \sim \Generate | I \sim D_1] \ge \gamma(1 - \epsilon)$, since $\Generate$ is close in total variation distance to $D_1$ and $\cA$ has advantage $\epsilon$ in finding $r$ when given input that is sampled from $\Generate(n,1)$.
	
	We similarly notice that $\Pr_{I \sim D}[I \sim \Generate(n,1) | I \sim D_0] \le \epsilon$, since $\Generate(n,1)$ can be at most $\epsilon$-close to $D_0$ ($D_1$ and $D_0$ are disjoint). Therefore, 
	\begin{align*}
	\Pr_{I \sim D}[\cB(I) = 0 | I \sim D_0] &= 1 - \Pr_{I \sim D}[\cB(I) = 1 | I \sim D_0]\\
	&= 1 - \Pr_{I \sim D}[\cB(I) = 1 | I \sim D_0 \land I \sim \Generate(n,1)] \cdot \Pr[I \sim \Generate(n,1) | I \sim D_0]\\
	&\ge 1- \epsilon
	\end{align*}
	Here we are also upper-bounding $\gamma$ by $1$ and we use the fact that if $I$ is not sampled from $\Generate(n,1)$, then there is no randomness that inverts the function, and $\cB$ cannot possibly output $1$.
	
	Putting this back together, we have
	\begin{align*}
	\Pr_{I \sim D}[\cB(I) \mbox{ distinguishes }] &= \Pr_{I \sim D}[\cB(I) = 1 | I \sim D_1] \cdot \Pr_{I \sim D}[I \sim D_1] + \Pr_{I \sim D}[\cB(I) = 0 | I \sim D_0] \cdot \Pr_{I \sim D}[I \sim D_0]\\
	&= \frac 1 2 \left(\gamma(1 - \epsilon) + (1 - \epsilon) \right)
	\end{align*}
	
	Finally, we need to show that given our value for $\gamma \ge \frac{1 + 3\epsilon}{3(1 - \epsilon)}$, this value is at least $2/3$:
	\begin{align*}
	\Pr_{I \sim D}[\cB(I) \mbox{ distinguishes }] &\ge \frac 1 2 \left(\gamma(1 - \epsilon) + (1 - \epsilon) \right)\\
	&\ge \frac 1 2 \left( \left( \frac{1 + 3 \epsilon}{3(1 - \epsilon)} \right) \cdot (1 - \epsilon) + 1 - \epsilon \right)\\
	&= \frac 1 2 \cdot \frac 4 3 = \frac 2 3.
	\end{align*}
	
	So, assuming $P$ is $T(n)$-\ACIH, no adversary has better than a constant chance less than 1 of being able to invert $\Generate(n,1; r)$ is a medium $T(n)$-FGOWF. By claim \ref{thm:medium-strong-owf}, this implies there exist strong $T(n)$-FGOWFs.
\end{proof}


Note that $k$-SUM-$R$ and  Zero-$k$-Clique-$R$ are plantable with error less than $<5/12$ the when $R>6 n^k$ by Theorem \ref{thm:zkcPlantable} and Theorem \ref{thm:ksumPlantable}, these are both plantable and therefore can be used to build these fine-grained OWFs.

\subsection{Fine-Grained Hardcore Bits and Pseudorandom Generators}\label{sec:fg-hardcore-bits}
One way functions serve as the building block for a lot of symmetric encryption, and are (usually) implied by any other cryptographic primitive, from collision-resistant hash functions to symmetric-key encryption, to any flavor of public key encryption, and so on. The next step to building more cryptographic primitives with one-way functions is to see if we can use them to construct pseudorandom generators. While we do not yet have a construction of a fine-grained pseudorandom generator that can generate some sub-polynomial many pseudorandom bits\footnote{Note that due to the nature of being fine-grained, we cannot generate polynomially-many bits without additional assumptions}, we take the first steps, showing how to get hardcore bits.

\begin{definition}\label{def:fg-hcb}
	A function $b$ is a fine-grained hardcore (FGHC) predicate for a $T(n)$-FGOWF if for all $\PFT{T(n)}$ adversaries $\cA$,
	\[ \Pr_{x \getsr \{0,1\}^n}[ \cA(f(x)) = b(x) ] \le \frac 1 2 + \insig(n). \]
\end{definition}

Recall that in traditional cryptography, any OWF implies the existence of another OWF with a hardcore bit with the Goldreich-Levin (GL) construction \cite{hardCoreBitsAndXorLemmaFromGL}. The bad news: the GL construction required a security reduction with $O(n)$ evaluations of the one-way function. Given how we define problems to be $T(n)$-\ACIH~hard, this security reduction would not be $\PFT{T(n)}$.


\begin{theorem}[Fine-Grained Goldreich-Levin]\label{thm:fine-grained-GL}
	Let $f$ be an $\fgowf{T(n)}$. acting on strings $(x, y)$, where $x = n$ and $y = Q(n)$ for some subpolynomial $Q$, and let $\cA$ be a $\PFT{T(n)}$ such that
	\[\Pr[\cA(f(x,y), y) \in f^{-1}(f(x,y))] \ge \sig(n).\]
	Then, the function $f':~(x,y,r) \mapsto f(x,y)||r$ where $|r| = |y|$ has the hardcore bit $y \cdot r$.
\end{theorem}
\begin{proof}	
	Here we just trace through the GL reduction and show that as long as $|y|$ is sub-polynomial, the reduction will go through.
	
	First, the size of $y$, $Q(n)$, cannot be any subpolynomial, it must be large enough so that it is as hard to guess $y$ as it is to invert $f$ (because guessing $y$ yields a significant chance of inverting $f$). If $f$ is $T(n)$-hard to invert, then the time it takes to randomly guess bits, $2^{Q(n)}$, must be at least $T(n)$. Since $T(n)$ is at least linear, we can assume $Q(n) \ge \log(n)$.
	
	For a contradiction, assume that a $\PFT{T(n)}$ adversary $\cA$ has a significant advantage $\epsilon$ in determining $r \cdot y$ when given $f(x,y), r$. We will show this implies $\cB$, a $\PFT{T(n)}$ algorithm using $\cA$, can invert $f$ with significant probability.
	
	$\cB$ behaves as follows with parameter $m = 2Q(n)/\epsilon$ on input $x' = f(x,y)$:
	\begin{itemize}
		\item For every $i \in [Q(n)]$:
		\begin{enumerate}
			\item Choose $\log(m)$ pairs $(b_1, r_1), \dots, (b_{\log(m)}, r_{\log(m)}) \getsr \{0,1\} \times \{0,1\}^{Q(n)}$.
			\item For every $I$ in the powerset of $[\log(m)]$, let $(b'_I, r'_I) = \sum_{i \in I}(b_i, r_i)$.
			\item Let $r'_I \gets e_i \xor r_I$.
			\item Let $g_I \gets b'_I \xor \cA(x'||r_I)$
			\item Let $y'_i = \mathsf{Majority}({g_I})$
		\end{enumerate}
		\item Output $y' = y'_1, \dots, y'_{Q(n)}$.
	\end{itemize}
	
	First, consider the set $S = \{ (x,y) | \Pr_r[\cA(f(x,y)||r) = r\cdot y] \ge \frac \epsilon 2 \}$. A quick calculation yields $|S| > \epsilon\cdot 2^{n-1}\epsilon$.
	
	So, assume that our input $(x,y) \in S$. Now, assume that every pair we chose in step 1 has the property $b_1 = r_1 \cdot y$ (we that we correctly guessed the bit in question). This event occurs with probability $1/m$.
	
	Next, notice that each pair of $r'_I,$ and $r'_{I'}$ are independent, and so the whole set is pairwise independent. So, if $(x, y)\in S$, $\cA$ will return the correct bit given $f(x,y)||r_I$ at least $\epsilon/2$-fraction of the time for independent $r$'s. Because of the pairwise independence, a Chebyshev bound yields $\cA$ will return the correct bit a majority of the time after the $m$ queries (so $\mathsf{Majority}(\{g_I\})$ outputs the correct bit $y_i$) with probability at least $1-\frac 1 {m(\epsilon/2)^2}$.
	
	Finally, we put all of these pieces together to get
	\begin{align*}
	\Pr[\cB(f(x,y)) = y] & \ge \Pr[\cB(f(x,y)) = y | (x,y)\in S ] \cdot \frac \epsilon 2\\
	&= \frac \epsilon 2 \left( 1 - \Pr[\exists i \mbox{ s.t. } y_i \neq y'_i | (x,y) \in S] \right)\\
	&\ge \frac \epsilon 2 \left(1 - Q(n)\Pr[y_i \neq y'_i | (x,y) \in S]\right)\\
	&\ge \frac \epsilon 2 \left(1 - Q(n)\Pr[y_i \neq y'_i | (x,y) \in S \land \mbox{guessed all $b_i$ correctly}]\cdot \frac 1 m\right)\\
	&\ge \frac \epsilon 2 \left(1 - \frac {Q(n)}{m} \cdot \frac{1}{m(\epsilon/2)^2} \right)\\
	&= \frac \epsilon 2 - \frac{4Q(n)}{m^2\epsilon}
	\end{align*}
	
	Recall we set $m = 2Q(n)/\epsilon$, and since $\epsilon$ is significant and $Q$ is subpolynomial, $m$ is also subpolynomial. Importantly, because $\cB$ runs in $O(Q(n) \cdot m T(n)^{1-\delta})$, $\cB$ is $\PFT{T(n)}$.
	
	Therefore, the probability that $\cB$ succeeds in finding $y_i$ (and hence inverting $f(x,y)$ with significant probability), with probability $\frac \epsilon 2 - \frac{4 Q(n)\epsilon}{4Q^2(n)} \ge  \frac \epsilon 2 - \frac{4\epsilon}{Q(n)}$. Recall that $Q(n)$ is at least linear in $n$, and so we can assume $Q(n) > 16$. This implies the probability $\cB$ succeeds is $\frac \epsilon 4$. 
	
	Because $\epsilon$ is significant, $\cB$ breaks the fine-grained one-wayness of $f$. This is a contradiction. Therefore, $\epsilon$ must be insignificant.
\end{proof}

\subsubsection{Hardcore bits from $k$-SUM and Zero $k$-Clique}
For both of these problems, planting a solution is exactly choosing some number of values ($k$ for $k$-SUM, and the edge weights of a $k$-clique for Zero $k$-Clique) and  changing one of them so that the values now give a solution.

%TODO: THIS NEEDS A SERIOUS REWRITE.
\begin{corollary}
	Assuming either the Weak $k$-SUM hypothesis or weak Zero $k$-Clique hypothesis, there exist FGOWFs with fine-grained hardcore bits.
	\label{cor:hardcorebitkclique}
\end{corollary}
\begin{proof}
	This is straightforward due to the nature of planting for both of these hypotheses. Informally, planting for these problems is choosing a location within the given instance to put a solution. If an adversary learns where that solution is supposed to be, generating an instance without that specific solution is easy.
	
	First, let's prove this for $k$-SUM. The reason $k$-SUM is plantable is because $\Plant$ takes as its randomness $k$ indices in the $k$-SUM instance, and then changes the value one of them to make those $k$ instances form a solution the $k$-SUM. This randomness requires specifying $k$ instances out of $kn$, and an edge-weight. let $y$ be the $k\log(n)$ bits required to describe all $k$ locations that the planted solution lives; $y$ is part of the randomness in this case and we let $r$ denote which of these locations was changed. We can construct the following algorithm that inverts $\Plant(I; y, r)$ when given $y$.
	
	Let $\cB(I' = \Plant(I; y,r), y):$
	\begin{enumerate}
		\item Let $y[1] \in A_1$ denote the entry in list $A_1$ that is part of the $k$-SUM solution.
		\item Let $\hat I$ be $I'$ except with $y[0]$ changed to a different random value in the range $[0, R-1]$.
		\item Output $(\hat I; y, 1)$.
	\end{enumerate}
	
	$\cB$ runs in time $\tilde O(n)$, and is correct. The correctness comes from the fact that we preserve a zero-sum. $\Plant(\hat I; y, 1)$ will output an $I''$ such that $I''$ is just $\hat I$ with one entry changed, $y[1]$. $\hat I$ is the same as $I'$ but also with the same entry changed, $y[1]$. We need to show they are changed to the same value. In both $I''$ and $I'$, $\sum y[i] \equiv 0 \mod R$, and so $y[1] = \sum_{i=2}^k y[i] \mod R$. Because $y[2], \dots, y[k]$ are the same values across $I', \hat I,$ and $I''$, $y[1]$ is the same value in both $I'$ and $I''$. Finally, because we have a sub-polynomially sized advice to give an adversary for a $\PFT{n^k}$ adversary to produce a preimage, the fine-grained GL variant applies to the one-way function $\Plant$.
	
	The proof that Zero $k$-Clique has this property is analogous: planting chooses a specific location, describable in $\poly(\log(n))$ bits, to plant a clique. Knowing where the clique was planted allows one to quickly produce something in the preimage of a given instance.
	\qed
\end{proof}

%\xxx{Rio: To deal with at a later point. Maybe just some discussion.}
%\begin{claim}
%	Suppose $f$ is a medium or strong $\fgowf{T(n)}$ acting on $(x,y)$ where $x = n$ and $y = Q(n)$ for some subpolynomial $Q$, and let $b$ be a predicate on $(x,y)$ such that for all $\PFT{T(n)}$ adversaries $\cA$, $\Pr[\cA(f(x,y)) = b(x,y)] \le \frac 1 2 + \frac 1 {Q'(n)}$ for any subpolynomial $Q'$, then there exists a strong $\fgowf{T(n)}$ $f'$ with a strong hardcore bit $b'$.
%\end{claim}
%\begin{proof}
	%	This is just an application of the computational XOR lemma \cite{Vaz87} in combination with the techniques from claim \ref{thm:medium-strong-owf}.
	%	
	%	Recall the technique from claim \ref{thm:medium-strong-owf}: $f'$ consisted of concatenating $f$ together many times. How many times depends on whether or not $f'$ is easy to invert. If $\Pr[\cA(f(x)) \in f\inv(f(x))] = 1 - \frac 1 {\hat Q(n)}$ (so $f$ could be medium or strong since in both cases $\hat Q$ is sub-polynomial), then let $Q^*(n) = \max(Q'(n), \hat Q(n)) \cdot \log^2(n)$. Let $f' = f || \dots || f$, concatenated $Q^*(n)$ times. Then, $f'$ is definitely now a strong $\fgowf{T(n)}$, and also will have the following hardcore bit $b'$:
	%	\[b'((x_1, y_1) \dots, (x_{Q^*(n)}, y_{Q^*(n)})) = b(y_1) \xor \dots \xor b(y_{Q^*(n)}).\]
	%	
	%	Any $\PFT{T(n)}$ algorithm $\cA$ will be able to distinguish the vector $(b(y_1), \dots, b(y_{Q^*(n)})$
	%	\xxx{Rio: I'm having a very tough time figuring this out for whatever reason.}
	%TODO: figure this out
%\end{proof}
%%
%\paragraph{A note on fine-grained pseudorandom generators.}
%\xxx{Rio: I'm still looking into this. It's complicated...}
%TODO: constructions from GL and hc bits

%TODO: Hill construction? What about https://people.seas.harvard.edu/~salil/research/OWFtoPRG-stoc10.pdf

%\begin{definition}
%	A function $f$ is a nearly-weak $k,k'$-FGOWF if there exists a function $Q(n) \le O(n^{k-k'-\delta})$ for a constant $\delta > 0$ such that for all $\PFT{n^k}$ adversaries $\cA$,
%	\[ \Pr_{x \getsr \{0,\}^n}[ \cA(f(x)) \in f\inv(f(x)) ] \le 1 - \frac 1 {Q(n)}. \]
%\end{definition}
%
%\begin{claim}
%	Nearly-weak $k,k'$-FGOWFs imply strong $k$-FGOWFs.
%\end{claim}
%\begin{proof}
%	%TODO: it's gonna look like the previous proof
%\end{proof}
%
%%\remove{
%\xxx{There are some problems with these definitions. Need to work out where $\delta'$ comes in etc.}
%
%Just like in the usual cryptographic setting we have weak and strong OWFs (and weak OWFs imply strong OWFs), we have our own fine-grained version of weak and strong OWFs -- one will imply the other. There will be two versions: a `gap preserving' one-way function we will call a medium OWF, and an $\epsilon$-weak OWF.
%
%%TODO: there are some problems with these definitions...
%
%\paragraph{Definitions } 
%These functions will define the $\epsilon(n,\delta')$ to be different functions. In our first definition $\epsilon$ won't even depend on $\delta'$. It asks that the adversary have a sub-polynomial error in $n$.
%
%\begin{definition}[$d$-medium one-way functions]
%	A function $f: \{0,1\}^n \to \{0,1\}^*$ is $d$-medium one-way if $f(x)$ can be computed in $O(t(n))$ time and for all adversaries $\cA$ taking $O(t(n)^{g-\delta'})$ time for some $\delta'>0$, there exists a sub-polynomial function $
%	\epsilon(n)$ such that,
%	\[ \Pr_{x \gets \{0,1\}^n}\left[ \cA(f(x)) \in f\inv(x) \right] \le 1 - \frac 1 {\epsilon(n)} \]
%	%TODO: where does \delta' factor into this definition?
%	%%Andrea: I don't think it does. I think that we beleive that we can construct functions where, when \delta'>0 for any delta, the success is thus bounded. 
%\end{definition}
%
%In the above definition we force the adversary to err quite a bit. What about adversaries that err only $\frac{1}{\text{poly}(n)}$? Well, given that computing $f(n)$ takes $t(n)$ time and inverting $f(n)$ takes $t(n)^g$ time if the adversary errs, for example, $\frac{1}{t(n)^{g+1}}$ of the time, we could not detect this in $t(n)^g$ time. 
%
%But, the difference between inverting and solving gives us some space to work with, and in that space we can have certain amounts of error. 
%
%\begin{definition}
%	A function $f: \{0,1\}^n \to \{0,1\}^*$ is $(d,\epsilon)$-weak one-way if $f(x)$ can be computed in $O(t(n))$ time and for all adversaries $\cA$ taking $O(t(n)^{d-\delta'})$ time, there exists a function $g(n) = o(t(n)^{d-1-\epsilon})$ such that
%	\[ \Pr_{x \gets \{0,1\}^n}\left[ \cA(f(x)) \in f\inv(x) \right] \le 1 - \frac 1 {g(n)} \]
%\end{definition}
%
%\paragraph{Implications } 
%\begin{theorem}
%	The existence of a $d$-medium one-way function, $f$, implies the  existence of a $(d, n^{-c})$-one-way function, $f'$, for all constants $c$.
%\end{theorem}
%\begin{proof}
%	Let the probability of correct inversion of $f$ be upper bounded by $1-\frac 1 {g(n)}$. 
%	
%	We can draw $2cg(n)\lg(n)$  independent samples $x_1, \ldots , x_{2cg(n)\lg(n)}$ from $\{0,1\}^n$. Now, our new one way function is $f'(x_1|x_2| \ldots |x_{2cg(n)\lg(n)}) = f(x_1)||f(x_2)||\ldots||f(x_{2cg(n)\lg(n)})$. Correctly inverting the function $f'$ requires inverting $2cg(n)\lg(n)$ samples. The probability of correct inversion of this new function will be 
%	$(1-\frac 1 {g(n)})^{2g(n)\cdot c\lg(n)} < $
%	
%\end{proof}
%
%\begin{theorem}
%	The existence of a $(d,\epsilon)$-weak one-way function implies the  existence of a $(d/(d-\epsilon), n^{-c})$-one-way function for all constants $c$.
%\end{theorem}
%\begin{proof}
%	
%	\xxx{We build this proof by doing many repetitions of the medium one way functions. Repeating the function $cg(n)\lg(n)$ times achieves this while preserving the polynomial gap.}
%\end{proof}

%\xxx{Rio: removed explicit discussion about explicit $k$-SUM construction}
%\subsection{One Way Functions}

%\begin{theorem}
%If an problem is hypothesized to take  $\tilde{\Omega}(t(n)^{1+\delta})$ time for some $\delta>0$ time to find a solution with probability $\geq 1/3$  and is plantable in $t(n)$ time with error $< 2/3$ then a fine grained one way function exists. 
%\end{theorem}
%\begin{proof}
%	\xxx{TODO: but you know, its not so hard to prove this}
%\end{proof}

%Given $I,(a,b,c)$ and, $(w_{a,b}, w_{b,c})$ let $I'$ be an instance of $I$ with three edge weights changed. Specifically, set $w(e_{a,b})= w_{a,b}$, $w(e_{b,c})= w_{b,c}$ and, $w(e_{c,a})= -w_{a,b}-w_{b,c}$.
%
%\begin{definition}
%Let $I$ be an instance of $0$ k-clique
%Let $f_{kClique}$ be the function 
%$$f_{kClique}(I,(a,b,c),(w_{a,b}, w_{b,c})) =
%\begin{cases}
%\emptyset , & \text{if }e(a,b), e(b,c),\text{or } e(c,a) \text{ are involved in a }0 \text{-triangle in I} \\
%\emptyset , & \text{if }e(a,b), e(b,c),\text{or } e(c,a) \text{ are involved in }> \text{1  }0 \text{-triangles in I'} \\
%I', & \text{else}
%\end{cases}
%$$
%\end{definition}
%
%\xxx{TODO: can we get away with not checking for the null characters? It will still be hard over the distribution when the inputs are average case 0 solutions. Then computing $f$ will be $O(n^2)$ this doesn't matter unless $k>4$.}
%
%\begin{theorem}
%The function $f_{kClique}$ is a fine-grained one way function over the distribution of inputs drawn uniformly from $k$-Clique instances with $0$ solutions given the average case $0$-k Clique assumption. 
%\label{thm:kcliqueOWF}
%\end{theorem}
%\begin{proof}
%Computing $f_{kClique}$ takes $n^{k-2}$ time when the instance $I$ has $n$ nodes. 
%
%Let $D_I$ be the uniform distribution over $0$-k clique instances with $0$ solutions. \\
%Let $D_{a,b,c)}$ be the uniform distribution of triples with each value chosen uniformly from $[1,n]$.\\
%Let $D_{w}$ be the distribution from Lemma \ref{lem:kcliquePlant} that generates average case solutions. \\
%Let $D_{in}$ be the distribution made by sampling $I$ from $D_I$, $(a,b,c)$ from $D_{(a,b,c)}$ and $(w_{a,b},\ldots)$ from $D_{w}$.
%
%Let the Adversary have a $\epsilon$ probability of inverting the function when the input to the function is chosen from $D_{in}$.
%
%We will then use the adversary to distinguish a random $0$ solution instance from a $1$ solution instance. If we are given an instance $\hat{I}$ then we give the adversary this instance and ask for an inversion. 
%
%If the instance has $0$ solutions the adversary will be unable to invert (no $(a,b,c)$ triple will form a $0$-triangle). If the instance has $1$ solution, this is by Lemma \ref{lem:kcliquePlant} going to be the same distribution as applying the function to the distribution from $D_{in}$. 
%
%Thus, the Adversary will invert the function and find the $0$-k clique an $\epsilon$ fraction of the time. If the Adversary fails we will guess $0$ solutions, if the Adversary succeeds we will guess $1$ solution. 
%
%Given that  $\hat{I}$ has a $1/2$ chance of being a $0$ input then we guess correctly $1/2+\epsilon$ of the time. 
%
%Thus, the Adversary can not invert the function correctly with probability $\geq 1/3$.
%\end{proof}