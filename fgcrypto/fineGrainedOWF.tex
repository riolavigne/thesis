\section{Fine-Grained One-Way Functions}\label{sec:fg-owfs}

In this section, we give a construction of fine-grained OWFs (FGOWF) based on plantable $T(n)$-\ACIH~problems. We first show that even though the probability of inversion may be constant (we call this ``medium'' fine-grained one-way), we can do some standard boosting in the same way weak OWFs can be transformed into strong OWFs in the traditional sense. Then, given such a plantable problem, we will prove that $\Generate(n,1)$ is a medium $T(n)$-FGOWF. Then, from this medium FGOWF, we can compile a strong FGOWF using this boosting trick. Then, since \zkclique~is plantable (see Theorem \ref{thm:zkcPlantable}), this implies that assuming \zkclique~is hard yields fine-grained OWFs.

Finally, we discuss the possibility of fine-grained hardcore bits and pseudorandom generators. It turns out that the standard Goldreich-Levin \cite{hardCoreBitsAndXorLemmaFromGL} approach to creating hardcore bits works in a similar fashion here, but requires some finessing; it will not work for \emph{all} fine-grained OWFs.

We will be using $\tilde{O}(\cdot)$ to suppress sub-polynomial factors of $n$ (as opposed to only $\lg(n)$ factors). 

\subsection{Weak and Strong OWFs in the Fine-Grained Setting}

Traditional cryptography has notions of weak and strong OWFs. Weak OWFs can be inverted most of the time, but a polynomial-fraction of the time, they cannot be. These weak OWFs can be compiled into strong OWFs (showing that weak OWFs imply strong OWFs), where there is a negligible chance that the resulting strong OWF is invertible over the choice of inputs.

Here we will briefly define ``medium'' $T(n)$-FGOWFs, and show how they can imply a ``strong'' $T(n)$-FGOWF, where ``strong'' refers to definition \ref{def:fg-owf}

\begin{definition}
	A function $f$ is a medium $T(n)$-FGOWF if there exists a sub-polynomial function $Q(n)$ such that for all $\PFT{T(n)}$ adversaries $\cA$,
	\[ \Pr_{x \getsr \{0,1\}^n}\bigl[ \cA(f(x)) \in f\inv(f(x)) \bigr] \le 1 - \frac 1 {Q(n)}. \]
\end{definition}

\begin{claim}\label{clm:medium-strong-owf}
	Medium $T(n)$-FGOWFs imply strong $T(n)$-FGOWFs for any polynomial $T(n)$ that is at least linear.
\end{claim}
\begin{proof}
	The structure of this proof will follow Yao's original argument augmenting weak OWFs to strong ones. Intuitively, we are able to use this argument because sub-polynomial functions compose well with each other.
	
	Assume for the sake of contradiction that no strong $T(n)$-FGOWF exists. Then, there exists some function $p’(n)$ such that $p’(n)$ is sub-polynomial and for all functions $f$ computable in $T(n)^{1-\epsilon}$ time there exists a $PFT_{T(n)}$ adversary that can invert the function $f$ with probability $1-\frac{1}{p’(n)}$.
	
	Let $f$ be a medium $T(n)$-FGOWF, where the probability any $\PFT{T(n)}$ adversary inverts it is $1 - \frac 1 {Q(n)}$. The basic idea will be to produce $g$ that is just a concatenation of many $f$s, just as in the traditional cryptographic case.
	
	For any positive-integer function $c(n)$, let $g(x_1 || \dots || x_{c(n)}) = f(x_1)||$ $\dots ||f(x_{c(n)})$, where $||$ denotes concatenation.
	Let $c(n) = 4 \left( Q(n)r(n) \right)^2$ (or the ceiling of $4 \left( Q(n)r(n) \right)^2$, if not an integer). 	Where $r(n)$ is a subpolynomial function such that $r(n) \ge p’(c(n) \cdot n)$. Note that $p’(n)$ is subpolynomial, and that as a result some such function $r(n)$ exists. Specifically, setting $r(n) = p'(n^2)$ satisfies both criteria.
	
	Now note that $c(n) \cdot n = \~O(n)$, and so $T(c(n) \cdot n) = \~O(T(n))$. Furthermore, $g$ is a $T(c(n) \cdot n) = O(T(n))$-FGOWF since $c(n)$ is subpolynomial.
	
	Now, for sake of contradiction, let $\cA$ be a $\PFT{T(n)}$ such that there exists a subpolynomial function $p'$ where
	\[\Pr\bigl[\cA(g(x_1 || \dots || x_c)) \in g^{-1}(g(x_1 || \dots || x_c))\bigr] \ge \frac 1 {p'(c(n) \cdot n)}.\]
	
	
	Let $p(n) = p'(c(n) \cdot n)$. Because $c(n)\cdot n = O(n^2)$ and $p'(n)$ is sub-polynomial, $p'(c(n) \cdot n) = p(n)$ is also sub-polynomial. Therefore,
	\[\Pr\bigl[\cA(g(x_1 || \dots || x_c)) \in g^{-1}(g(x_1 || \dots || x_c))\bigr] \ge \frac 1 {p(n)}.\]

	We will define a $\PFT{T(n)}$ function $\cA_0$ that makes a single call to $\cA$: on input $y = f(x)$
	\begin{enumerate}
		\item Choose $i \getsr [c(n)]$.
		\item Let $z_i = y$
		\item For all $j \in [c(n)]$, $j \neq i$, $x_j \getsr \{0,1\}^n$ and $z_j = f(x_j)$.
		\item Run $\cA$ on $(z_1, \dots, z_{c(n)})$ to get output $(x_1, \dots, x_{c(n)})$ if $\cA$ succeeds.
		\item If $\cA$ succeeded, output $x_i$.
	\end{enumerate}
	
	Because all operations in $\cA_0$ are either calling $\cA$ (once) or take time $O(n\cdot c(n))$,\footnote{It does not make much sense for $T(n)$ to be sublinear for our contexts} $\cA_0$ is a $\PFT{T(n)}$ algorithm. Now, we will let $\cB$ be an algorithm calling $\cA_0$ $d(n) = 4c(n)^2 (n)p(n)Q(n)$ times, returning a valid inversion of $f(x)$ if $\cA$ succeeded at least once.
	
	We will call $x \in \{0,1\}^n$ 'good' if $\cA_0$ inverts it with probability at least $\frac 1 {2c^2(n)p(n)}$; $x$ is 'bad' otherwise. Notice that if $x$ is good, then $\cB$, which runs $\cA$ many times, succeeds with high probability:
	
	\[ \Pr\left[\cB(f(x)) \mbox{ fails} | x\mbox{ is good}\right] \le \left(1 - \frac 1{2c^2(n)p(n)}\right)^{d(n)} \sim e^{-2Q(n)} < \frac 1 {2Q(n)}. \]
	
	
	We will show that there are at least $2^n(1 - \frac{1}{2p(n)})$ good elements.
	
	
	\begin{claim}
		There are at least $2^n(1 - \frac{1}{2p(n)})$ good elements.
	\end{claim}
	\begin{proof}
		For a contradiction, assume there are at least $2^n(\frac{1}{2p(n)})$ bad elements. We will end up contradicting the inversion probability of $\cA$ (which is at least $1/p(n)$). For notation, let $\vec x = (x_1, \dots, x_{c(n)}) \in \{0,1\}^{n\cdot c(n)}$, and $\vec x$ will be chosen uniformly at random over the input space.
		\begin{align*}
		\Pr_{\vec x}[\cA(\vec z = g(\vec x)) \mbox{ succeeds}] &= \Pr[\cA(\vec z)\mbox{ succeeds} \land \exists \mbox{bad }x_j]\\
		&\qquad + \Pr\left[\cA(\vec z)\mbox{ succeeds} \land \mbox x_j \mbox{ good }\forall j \in [c(n)] \right]
		\end{align*}
		
		Now, for all $j \in [c(n)]$,
		\begin{align*}
		\Pr_{\vec x}[\cA(\vec z) \mbox{ succeeds} \land x_j\mbox{ is bad}] &\le \Pr_{\vec x}[\cA(\vec z) \mbox{ succeeds} |  x_j\mbox{ is bad}]\\
		%\cdot \Pr_{x_j}[ x_j\mbox{ is bad}]\\
		&\le c(n) \Pr_{\vec x}[\cA_0(f(x_j))\mbox{ succeeds} | x_j\mbox{ is bad}]\\
		&\le \frac{c(n)}{2c^2(n)p(n)} = \frac{1}{2c(n)p(n)}
		\end{align*}
		So, if we just union bound over all $j$, we get
		\[ \Pr_{\vec x}[\cA(\vec z) \mbox{ succeeds} \land \mbox{some } x_j \mbox{ are bad}] \le \sum_{j = 1}^{c(n)} \Pr_{\vec x}[\cA_0(f(x_j))\mbox{ succeeds} \land x_j\mbox{ is bad}] \le \frac{1}{2p(n)}. \]
		

		
		And one more quick upper bound yields
		
		\begin{align*}
	\Pr[\cA(\vec z) \mbox{ succeeds} \land \mbox{all } x_j \mbox{ are good}] &\le \Pr_{\vec x}[\mbox{all $x_j$ good}]\\
	&< \left(1 - \frac{1}{2p(n)}\right)^{c(n)}\\
	&\leq e^{-2\left(Q(n)r(n)\right)^2 / p(n)}\\
	&\leq e^{- 2\left(Q(n)\right)^2 \cdot p(n)} < \frac 1 {2p(n)}.
		\end{align*}
		
		Finally, this yields the contradiction to the claim that there are at least $2^n(\frac{1}{2p(n)})$ bad elements:
		\begin{align*}
		\Pr_{\vec x}[\cA(\vec z) \mbox{ succeeds}] < \frac{1}{p(n)}.
		\end{align*}\qed
	\end{proof}

	Note that $p(n)\geq Q(n)$ because $1/Q(n)$ is the maximum probability of inverting a single copy of $f(\cdot)$, where as $1/p(n)$ is assumed (for contradiction) to be the probability that a function inverts $c(n)$ copies of $f(\cdot)$ simultaneously. So, there are at most $2^n(\frac{1}{2Q(n)})$ bad elements.

	Now that we know there is a high probability that we hit a good $x$, we can finish the rest of this proof.
	\begin{align*}
	\Pr_{x}[\cB(f(x)) \mbox{ fails}] &= \Pr[\cB(f(x)) \mbox{ fails} | x \mbox{ is good}] \Pr_x[x \mbox{ is good}]\\
	&\qquad + \Pr[\cB(f(x)) \mbox{ fails} | x \mbox{ is bad}] \Pr_x[x \mbox{ is bad}]\\
	&\le\Pr[\cB(f(x)) \mbox{ fails} | x \mbox{ is good}] \Pr_x[x \mbox{ is good}] + \Pr_x[x \mbox{ is bad}]\\
	&\le \frac{1}{2Q(n)}\left(1 - \frac 1 {2Q(n)}\right) + \frac{1}{2Q(n)} < \frac 1 {Q(n)}
	\end{align*}
	Thus, the chance that $\cB$ actually has of inverting $f$ is strictly greater than $1 - \frac 1 {Q(n)}$, contradicting the claim that $f$ was medium-hard with respect to $Q(n)$.\qed
\end{proof}

\paragraph{Weaker Fine-Grained OWFs.} Now, because we are in the fine-grained setting, we can talk about gaps. There is a notion of weak-OWFs in cryptography where we can say if there exists \emph{any} polynomial such that we can invert with probability $1 - 1/\poly$, we can construct strong OWFs. We want a similar notion for fine-grained OWFs. Here we can't just choose any polynomial --- we have to choose a polynomial that respects the gap.

Formally, for an $T(n)$-FGOWF $f$ that has $\PFT{T(n)}$ adversaries inverting it with probability $1 - 1/P(n)$ for some $P(n)$, we can get that a $\PFT{T(n)}$ adversary can invert $f$ with probability $(1 - 1/P(n))^{c(n)}$. Now, as long as there exists $\delta'$ such that $T(n)^{1-\delta}P(n) = T(n)^{1-\delta'}$, there is still a gap ($\delta' < \delta$) even if we compute $f$ $P(n)$ times to evaluate $f$. Therefore, we are able to get a strong fine-grained OWF from a weak one, as long as it's not \emph{too} weak.

\subsection{Building Fine-Grained OWFs from Plantable Problems}\label{sec:building-fgowfs}

Here we show that one can generate fine-grained one way functions from plantable problems. Recall the definition of Plantable states that there exists an algorithm $\Generate(n,b)$ where when $b=0$, an instance of the problem \emph{without} a solution is generated, and when $b=1$, an instance of a problem \emph{with one} solution is generated with probability at least $1 - \epsilon$. This probabilistic element, $\epsilon$, is actually a bound on the total variation distance of the distributions we are actually aiming to sample from: $\Generate(n,0)$ and $\Generate(n,1)$ have total variation distance at most $\epsilon$ from $D_0(P,n)$ and $D_1(P,n)$ respectively.

\begin{theorem}\label{thm:FGOWFs-exist}	%\label{thm:plantableOWF}
	If there exists a Plantable $T(n)$-\ACIH~problem where $G(n)$ is $\PFT{T(n)}$ with error some constant $\epsilon < 1/3$, then $T(n)$-FGOWFs exist.
	\footnote{We would like to thank Chris Brzuska and his reading group for finding a bug in the original version of this proof. To correct this bug, we added the word `constant' to the theorem statement, removed our severe abuse of notation, and fixed the proof accordingly.}
\end{theorem}
\begin{proof}
	Let $P$ be a Plantable $T(n)$-\ACIH~problem where $G(n) = T(n)^{1-\delta}$ for some constant $\delta>0$. So, the (randomized) algorithm $\Generate(n,1)$ is $\PFT{T(n)}$ and outputs an instance $I$ that has at least one solution --- we write this as $\Generate(n,1; r)$ when explicitly noting which randomness was used.
	
	We want to show that being able to invert $\Generate(n,1; r)$, over the distribution from $r$, in a fine-grained sense, as solving the \ACIH~problem $P$. Let $\epsilon$ be the upper bound on the total variation distance between $\Generate(n,1)$ and $D_1(P,n)$, as per Definition \ref{def:plantable}.
	
	For sake of contradiction, assume that no \emph{medium} $T(n)$-FGOWF exist. So, we can invert $\Generate(n,1)$ with any probability $1 - \frac 1 {Q(n)}$ for any sub-polynomial $Q(n)$. Let $\cA$ be a $\PFT{T(n)}$ algorithm that inverts $\Generate(n,1)$ with probability $\gamma > 1 - \frac{1}{\log(n)}$ (note that $\log(n)$ is significant). We will show that this violates the assumption that $P$ is a $T(n)$-\ACIH~problem.
	
	We now construct a $\PFT{T(n)}$ algorithm $\cB$ that distinguishes between $I \sim D_0(P,n)$ and $I \sim D_1(P,n)$ with probability greater than $2/3$, violating the hardness assumption on $P$.
	\begin{itemize}
		\item Given $I$ from distribution $D$, $\cB$ gives $I$ to $\cA$.
		\item $\cA$ outputs $r$.
		\item If $\Generate(n,1;r) == I$, output $1$. Otherwise, output $0$.
	\end{itemize}
	
	
	%Andrea Edit
	We will now compute the probability that $\cB$ distinguishes between inputs from $D_1$ and $D_0$. Recall that $D$ is just sampling with $D_0$ with probability $1/2$, and otherwise samples from $D_1$. For the sake of brevity let the notation $I \in D_0$ and $I\in D_1$ convey that $I$ is in the support of $D_0$ and the support of $D_1$ respectively. We have
	\begin{align*}
	\Pr_{I \sim D}[\cB(I)\mbox{ distinguishes }D_0\text{ from }D_1] &= \Pr_{I \sim D_1}[\cB(I) = 1] \cdot \Pr_{I \sim D}[I \in D_1] \\
	&\qquad + \Pr_{I \sim D_0}[\cB(I) = 0]\cdot \Pr_{I \sim D}[I \in D_0]\\
	&= \frac 1 2 \Pr_{I \sim D_1}[\cB(I) = 1] + \frac 1 2 \Pr_{I \sim D_0}[\cB(I) = 0].
	\end{align*}
	First, we note that $\Pr_{I \sim D_0}[\cB(I) = 0] = 1$ because $\Generate(n,1; r)$ is guaranteed to produce a witness for all randomness $r$. This means that any $I$ sampled from $D_0$ is not in the image of $\Generate(n,1)$, and therefore, $\cA$ cannot produce a valid inverse.
	
	Then, we use the fact that $D_1$ is close in total variation distance to $\Generate(n,1)$ to show that $\Pr_{I \sim D_1}[\cB = 1] \ge \gamma - 2\epsilon$. Let $p_{G_1}$ be the pdf of $\Generate(n,1)$ and $p_{D_1}$ be the pdf of $D_1$. Let $\cI$ be the set of all instances of the problem. Let $S = \{I \in \Img(\Generate(n,1)) : \Generate(n,1;\cA(I)) = I \}$ be the set of instances produced by $\Generate$ that $\cA$ can successfully invert. Recall that $\TVD(D_1, \Generate(n,1)) \leq \epsilon$ means $\sum_{I \in \cI} \vert p_D(I) - p_G(I) \vert \leq 2 \epsilon$ by the definition of $\TVD$.
	\begin{align*}
	2\epsilon &\ge \sum_{I \in \cI} \vert p_{D_1}(I) - p_{G_1}(I) \vert\\
	&\ge \sum_{I \in S} \vert p_{D_1}(I) - p_{G_1}(I) \vert \\
	&\ge \sum_{I \in S} \left[p_{D_1}(I) \right] - \sum_{I \in S} \left[p_{G_1}(I) \right].
	\end{align*}
	This implies $ \sum_{I \in S} \left[p_{G_1}(I) \right] \ge \sum_{I \in S} \left[p_{D_1}(I) \right] - 2\epsilon$, and therefore $\Pr_{I \sim D_1}[\cB = 1] \ge \gamma - 2\epsilon$.
	
		
	%Andrea Edit
    Notice that since $\epsilon$ is constant and less than $\frac 1 3$, $\frac 1 3 - \alpha = \epsilon$ for some constant $\alpha>0$.
	Putting this together, we have that
	\begin{align*}
	\Pr_{I \sim D}[\cB(I)\mbox{ distinguishes }D_0\text{ from }D_1] &\ge \frac 1 2 \cdot (\gamma - 2\epsilon) + \frac 1 2\\
	&= \frac \gamma 2 - \epsilon + \frac 1 2\\
	&= 1 - \frac{1}{2\log(n)} - \epsilon\\
	&\ge 1 - \frac{1}{2\log(n)} - \left(\frac 1 3 - \alpha \right)\\
	&> \frac 2 3.
	\end{align*}
 Note that $\frac 1 {2\log(n)}$ is less (asymptotically) that any constant $\alpha$, the sum of these terms is greater than $\frac 2 3$.
	
	So, assuming $P$ is $T(n)$-\ACIH, i.e. no adversary has better than a constant chance less than 1 of being able to invert $\Generate(n,1; r)$, then $\Generate$ is a medium $T(n)$-FGOWF. By Claim \ref{clm:medium-strong-owf}, this implies strong $T(n)$-FGOWFs exist.
	\qed
\end{proof}

Note that \kSum-$R$ and \zkclique-$R$ are plantable with error less than $1/3$ the when $R > 6 n^k$ by Theorem \ref{thm:zkcPlantable} and Theorem \ref{thm:ksumPlantable}, these are both plantable and therefore can be used to build these fine-grained OWFs.

\subsection{Fine-Grained Hardcore Bits and Pseudorandom Generators}\label{sec:fg-hardcore-bits}
One way functions serve as the building block for a lot of symmetric encryption, and are (usually) implied by any other cryptographic primitive, from collision-resistant hash functions to symmetric-key encryption, to any flavor of public key encryption, and so on. The next step to building more cryptographic primitives with one-way functions is to see if we can use them to construct pseudorandom generators. While we do not yet have a construction of a fine-grained pseudorandom generator that can generate some sub-polynomial many pseudorandom bits\footnote{Note that due to the nature of being fine-grained, we cannot generate polynomially-many bits without additional assumptions}, we take the first steps, showing how to get hardcore bits.

\begin{definition}\label{def:fg-hcb}
	A function $b$ is a fine-grained hardcore (FGHC) predicate for a $T(n)$-FGOWF if for all $\PFT{T(n)}$ adversaries $\cA$,
	\[ \Pr_{x \getsr \{0,1\}^n}[ \cA(f(x)) = b(x) ] \le \frac 1 2 + \insig(n). \]
\end{definition}

Recall that in traditional cryptography, any OWF implies the existence of another OWF with a hardcore bit with the Goldreich-Levin (GL) construction \cite{hardCoreBitsAndXorLemmaFromGL}. The bad news: the GL construction required a security reduction with $O(n)$ evaluations of the one-way function. Given how we define problems to be $T(n)$-\ACIH~hard, this security reduction would not be $\PFT{T(n)}$.


\begin{theorem}[Fine-Grained Goldreich-Levin]\label{thm:fine-grained-GL}
	Let $f$ be an $\fgowf{T(n)}$ acting on strings $(x, y)$, where $|x| = n$ and $|y| = Q(n)$ for some subpolynomial $Q$, and assume there exists a $\PFT{T(n)}$~algorithm $\cL$ such that
	\[\Pr[\cL(f(x,y), y) \in f^{-1}(f(x,y))] \ge \sig(n).\]
	Then, the function $f':~(x,y,r) \mapsto f(x,y)||r$ where $|r| = |y|$ has the hardcore bit $y \cdot r$.
\end{theorem}
\begin{proof}	
	Here we just trace through the GL reduction and show that as long as $|y|$ is sub-polynomial, the reduction will go through.
	
	First, the size of $y$, $Q(n)$, cannot be any subpolynomial, it must be large enough so that it is as hard to guess $y$ as it is to invert $f$ (because guessing $y$ yields a significant chance of inverting $f$). If $f$ is $T(n)$-hard to invert, then the time it takes to randomly guess bits, $2^{Q(n)}$, must be at least $T(n)$. Since $T(n)$ is at least linear, we can assume $Q(n) \ge \log(n)$.
	
	For a contradiction, assume that a $\PFT{T(n)}$ adversary $\cA$ has a significant advantage $\epsilon$ in determining $r \cdot y$ when given $f(x,y), r$. We will show this implies $\cB$, a $\PFT{T(n)}$ algorithm using $\cA$, can invert $f$ with significant probability.
	
	$\cB$ behaves as follows with parameter $m = 2Q(n)/\epsilon$ on input $x' = f(x,y)$:
	\begin{itemize}
		\item For every $i \in [Q(n)]$:
		\begin{enumerate}
			\item Choose $\log(m)$ pairs $(b_1, r_1), \dots, (b_{\log(m)}, r_{\log(m)}) \getsr \{0,1\} \times \{0,1\}^{Q(n)}$.
			\item For every $I$ in the powerset of $[\log(m)]$, let $b'_I = \sum_{j \in I}b_j \mod 2$.
			\item For every $I$ in the powerset of $[\log(m)]$, let $r_I = \sum_{j \in I}r_j \mod 2$.
			\item For every $I$ in the powerset of $[\log(m)]$,
			\begin{itemize}
						\item Let $s_I \gets e_i \xor r_I$ where $e_i$ is the $i^{th}$ standard basis vector ($e_i$ is all zeros except for one $1$ in the $i^{th}$ index).
						\item Let $g_I \gets b'_I \xor \cA(x'||s_I)$
			\end{itemize}
			\item Let $z_i = $ the Majority bit over all $2^{\log(m)}$ bits $g_I$.%\mathsf{Majority}($%\{g_I\}_{I \subset [\log(m)]})$
		\end{enumerate}
		\item Output $z = z_1, \dots, z_{Q(n)}$.
	\end{itemize}
	
	First, consider the set $S = \left\{ (x,y) | \Pr_r[\cA(f(x,y)||r) = r\cdot y] \ge \frac 1 2 + \frac \epsilon 2 \right\}$. A quick calculation yields $|S| > \epsilon\cdot 2^{n-1}$.
	
	So, assume that our input $(x,y) \in S$. Now, assume that every pair we chose in step 1 has the property $b_i = r_i \cdot y$ (we correctly guessed the bit in question). This event occurs with probability $1/m$.
	
	Next, notice that each pair of $s_I,$ and $s_{J}$ ($I \neq J$) are independent, and so the whole set is pairwise independent. So, if $(x, y)\in S$, $\cA$ will return the correct bit given $f(x,y)||r_I$ at least an $\epsilon/2$-fraction of the time for independent $r$'s. Because of the pairwise independence, a Chebyshev bound yields $\cA$ will return the correct bit a majority of the time after the $m$ queries (so $\mathsf{Majority}(\{g_I\})$ outputs the correct bit $y_i$) with probability at least $1-\frac 1 {m(\epsilon/2)^2}$.
	
	Finally, we put all of these pieces together to get
	\begin{align*}
	\Pr[\cB(f(x,y)) = y] & \ge \Pr[\cB(f(x,y)) = y | (x,y)\in S ] \cdot \frac \epsilon 2\\
	&= \frac \epsilon 2 \left( 1 - \Pr[\exists i \mbox{ s.t. } y_i \neq z_i | (x,y) \in S] \right)\\
	&\ge \frac \epsilon 2 \left(1 - Q(n)\Pr[y_i \neq z_i | (x,y) \in S]\right)\\
	&\ge \frac \epsilon 2 \left(1 - Q(n)\Pr[y_i \neq z_i | (x,y) \in S \land \mbox{guessed all $b_i$ correctly}]\cdot \frac 1 m\right)\\
	&\ge \frac \epsilon 2 \left(1 - \frac {Q(n)}{m} \cdot \frac{1}{m(\epsilon/2)^2} \right)\\
	&= \frac \epsilon 2 - \frac{4Q(n)}{m^2\epsilon}
	\end{align*}
	
	Recall we set $m = 2Q(n)/\epsilon$, and since $\epsilon$ is significant and $Q$ is subpolynomial, $m$ is also subpolynomial. Importantly, because $\cB$ runs in $O(Q(n) \cdot m T(n)^{1-\delta})$, $\cB$ is $\PFT{T(n)}$.
	
	Therefore, the probability that $\cB$ succeeds in finding $y_i$ (and hence inverting $f(x,y)$ with significant probability), with probability $\frac \epsilon 2 - \frac{4 Q(n)\epsilon}{4Q^2(n)} \ge  \frac \epsilon 2 - \frac{4\epsilon}{Q(n)}$. Recall that $Q(n)$ is at least linear in $n$, and so we can assume $Q(n) > 16$. This implies the probability $\cB$ succeeds is $\frac \epsilon 4$. 
	
	Because $\epsilon$ is significant, $\cB$ breaks the fine-grained one-wayness of $f$. This is a contradiction. Therefore, $\epsilon$ must be insignificant.
	\qed
\end{proof}

\subsubsection{Hardcore bits from \kSum~and \zkclique}
For both of these problems, planting a solution is exactly choosing some number of values ($k$ for \kSum, and the edge weights of a $k$-clique for \zkclique) and  changing one of them so that the values now give a solution.

\begin{corollary}
	Assuming either the Weak \kSum~hypothesis or weak \zkclique~hypothesis, there exist FGOWFs with fine-grained hardcore bits.
	\label{cor:hardcorebitkclique}
\end{corollary}
\begin{proof}
	This is straightforward due to the nature of planting for both of these hypotheses. Informally, planting for these problems is choosing a location within the given instance to put a solution. If an adversary learns where that solution is supposed to be, generating an instance without that specific solution is easy.
	
	First, let's prove this for \kSum. The reason \kSum~is plantable is because $\Generate(n,1)$ chooses $k$ indices at random in the \kSum~instance, and then changes the value one of them to make those $k$ instances form a solution the \kSum. This randomness requires specifying $k$ instances out of $kn$, and an edge-weight. Let $y$ be the $k\log(n)$ bits required to describe the $k$ locations of the solution; $y$ is part of the total randomness $r$ used in $\Generate(n,1)$. Without loss of generality, we can write $r = y||r'$. Let $f'(y||r', s) = \Generate(n,1; y||r')|| s$. Since $|y|$ is sub-polynomial, by Theorem \ref{thm:fine-grained-GL}, the bit $y \cdot s$ is hardcore for $f'$.
	
	Now, let's make the same argument for \zkclique. As before, $\Generate(n,1;r)$ can be written as $\Generate(n,1;y||r')$ where $y$ is the location of the zero $k$-clique generated. This location is just $k \cdot \log(n)$ bits; one coordinate from $n$ for each of the $k$ partitions in the graph. Therefore, we can define $f'(y||r', s) = \Generate(n,1;y||r') || s$, which, by Theorem \ref{thm:fine-grained-GL}, has the hardcore bit $y \cdot s$.
	\qed
\end{proof}

%\xxx{Rio: To deal with at a later point. Maybe just some discussion.}
%\begin{claim}
%	Suppose $f$ is a medium or strong $\fgowf{T(n)}$ acting on $(x,y)$ where $x = n$ and $y = Q(n)$ for some subpolynomial $Q$, and let $b$ be a predicate on $(x,y)$ such that for all $\PFT{T(n)}$ adversaries $\cA$, $\Pr[\cA(f(x,y)) = b(x,y)] \le \frac 1 2 + \frac 1 {Q'(n)}$ for any subpolynomial $Q'$, then there exists a strong $\fgowf{T(n)}$ $f'$ with a strong hardcore bit $b'$.
%\end{claim}
%\begin{proof}
	%	This is just an application of the computational XOR lemma \cite{Vaz87} in combination with the techniques from claim \ref{thm:medium-strong-owf}.
	%	
	%	Recall the technique from claim \ref{thm:medium-strong-owf}: $f'$ consisted of concatenating $f$ together many times. How many times depends on whether or not $f'$ is easy to invert. If $\Pr[\cA(f(x)) \in f\inv(f(x))] = 1 - \frac 1 {\hat Q(n)}$ (so $f$ could be medium or strong since in both cases $\hat Q$ is sub-polynomial), then let $Q^*(n) = \max(Q'(n), \hat Q(n)) \cdot \log^2(n)$. Let $f' = f || \dots || f$, concatenated $Q^*(n)$ times. Then, $f'$ is definitely now a strong $\fgowf{T(n)}$, and also will have the following hardcore bit $b'$:
	%	\[b'((x_1, y_1) \dots, (x_{Q^*(n)}, y_{Q^*(n)})) = b(y_1) \xor \dots \xor b(y_{Q^*(n)}).\]
	%	
	%	Any $\PFT{T(n)}$ algorithm $\cA$ will be able to distinguish the vector $(b(y_1), \dots, b(y_{Q^*(n)})$
	%	\xxx{Rio: I'm having a very tough time figuring this out for whatever reason.}
	%TODO: figure this out
%\end{proof}
%%
%\paragraph{A note on fine-grained pseudorandom generators.}
%\xxx{Rio: I'm still looking into this. It's complicated...}
%TODO: constructions from GL and hc bits

%TODO: Hill construction? What about https://people.seas.harvard.edu/~salil/research/OWFtoPRG-stoc10.pdf

%\begin{definition}
%	A function $f$ is a nearly-weak $k,k'$-FGOWF if there exists a function $Q(n) \le O(n^{k-k'-\delta})$ for a constant $\delta > 0$ such that for all $\PFT{n^k}$ adversaries $\cA$,
%	\[ \Pr_{x \getsr \{0,\}^n}[ \cA(f(x)) \in f\inv(f(x)) ] \le 1 - \frac 1 {Q(n)}. \]
%\end{definition}
%
%\begin{claim}
%	Nearly-weak $k,k'$-FGOWFs imply strong $k$-FGOWFs.
%\end{claim}
%\begin{proof}
%	%TODO: it's gonna look like the previous proof
%\end{proof}
%
%%\remove{
%\xxx{There are some problems with these definitions. Need to work out where $\delta'$ comes in etc.}
%
%Just like in the usual cryptographic setting we have weak and strong OWFs (and weak OWFs imply strong OWFs), we have our own fine-grained version of weak and strong OWFs -- one will imply the other. There will be two versions: a `gap preserving' one-way function we will call a medium OWF, and an $\epsilon$-weak OWF.
%
%%TODO: there are some problems with these definitions...
%
%\paragraph{Definitions } 
%These functions will define the $\epsilon(n,\delta')$ to be different functions. In our first definition $\epsilon$ won't even depend on $\delta'$. It asks that the adversary have a sub-polynomial error in $n$.
%
%\begin{definition}[$d$-medium one-way functions]
%	A function $f: \{0,1\}^n \to \{0,1\}^*$ is $d$-medium one-way if $f(x)$ can be computed in $O(t(n))$ time and for all adversaries $\cA$ taking $O(t(n)^{g-\delta'})$ time for some $\delta'>0$, there exists a sub-polynomial function $
%	\epsilon(n)$ such that,
%	\[ \Pr_{x \gets \{0,1\}^n}\left[ \cA(f(x)) \in f\inv(x) \right] \le 1 - \frac 1 {\epsilon(n)} \]
%	%TODO: where does \delta' factor into this definition?
%	%%Andrea: I don't think it does. I think that we beleive that we can construct functions where, when \delta'>0 for any delta, the success is thus bounded. 
%\end{definition}
%
%In the above definition we force the adversary to err quite a bit. What about adversaries that err only $\frac{1}{\text{poly}(n)}$? Well, given that computing $f(n)$ takes $t(n)$ time and inverting $f(n)$ takes $t(n)^g$ time if the adversary errs, for example, $\frac{1}{t(n)^{g+1}}$ of the time, we could not detect this in $t(n)^g$ time. 
%
%But, the difference between inverting and solving gives us some space to work with, and in that space we can have certain amounts of error. 
%
%\begin{definition}
%	A function $f: \{0,1\}^n \to \{0,1\}^*$ is $(d,\epsilon)$-weak one-way if $f(x)$ can be computed in $O(t(n))$ time and for all adversaries $\cA$ taking $O(t(n)^{d-\delta'})$ time, there exists a function $g(n) = o(t(n)^{d-1-\epsilon})$ such that
%	\[ \Pr_{x \gets \{0,1\}^n}\left[ \cA(f(x)) \in f\inv(x) \right] \le 1 - \frac 1 {g(n)} \]
%\end{definition}
%
%\paragraph{Implications } 
%\begin{theorem}
%	The existence of a $d$-medium one-way function, $f$, implies the  existence of a $(d, n^{-c})$-one-way function, $f'$, for all constants $c$.
%\end{theorem}
%\begin{proof}
%	Let the probability of correct inversion of $f$ be upper bounded by $1-\frac 1 {g(n)}$. 
%	
%	We can draw $2cg(n)\lg(n)$  independent samples $x_1, \ldots , x_{2cg(n)\lg(n)}$ from $\{0,1\}^n$. Now, our new one way function is $f'(x_1|x_2| \ldots |x_{2cg(n)\lg(n)}) = f(x_1)||f(x_2)||\ldots||f(x_{2cg(n)\lg(n)})$. Correctly inverting the function $f'$ requires inverting $2cg(n)\lg(n)$ samples. The probability of correct inversion of this new function will be 
%	$(1-\frac 1 {g(n)})^{2g(n)\cdot c\lg(n)} < $
%	
%\end{proof}
%
%\begin{theorem}
%	The existence of a $(d,\epsilon)$-weak one-way function implies the  existence of a $(d/(d-\epsilon), n^{-c})$-one-way function for all constants $c$.
%\end{theorem}
%\begin{proof}
%	
%	\xxx{We build this proof by doing many repetitions of the medium one way functions. Repeating the function $cg(n)\lg(n)$ times achieves this while preserving the polynomial gap.}
%\end{proof}

%\xxx{Rio: removed explicit discussion about explicit $k$-SUM construction}
%\subsection{One Way Functions}

%\begin{theorem}
%If an problem is hypothesized to take  $\tilde{\Omega}(t(n)^{1+\delta})$ time for some $\delta>0$ time to find a solution with probability $\geq 1/3$  and is plantable in $t(n)$ time with error $< 2/3$ then a fine grained one way function exists. 
%\end{theorem}
%\begin{proof}
%	\xxx{TODO: but you know, its not so hard to prove this}
%\end{proof}

%Given $I,(a,b,c)$ and, $(w_{a,b}, w_{b,c})$ let $I'$ be an instance of $I$ with three edge weights changed. Specifically, set $w(e_{a,b})= w_{a,b}$, $w(e_{b,c})= w_{b,c}$ and, $w(e_{c,a})= -w_{a,b}-w_{b,c}$.
%
%\begin{definition}
%Let $I$ be an instance of $0$ k-clique
%Let $f_{kClique}$ be the function 
%$$f_{kClique}(I,(a,b,c),(w_{a,b}, w_{b,c})) =
%\begin{cases}
%\emptyset , & \text{if }e(a,b), e(b,c),\text{or } e(c,a) \text{ are involved in a }0 \text{-triangle in I} \\
%\emptyset , & \text{if }e(a,b), e(b,c),\text{or } e(c,a) \text{ are involved in }> \text{1  }0 \text{-triangles in I'} \\
%I', & \text{else}
%\end{cases}
%$$
%\end{definition}
%
%\xxx{TODO: can we get away with not checking for the null characters? It will still be hard over the distribution when the inputs are average case 0 solutions. Then computing $f$ will be $O(n^2)$ this doesn't matter unless $k>4$.}
%
%\begin{theorem}
%The function $f_{kClique}$ is a fine-grained one way function over the distribution of inputs drawn uniformly from $k$-Clique instances with $0$ solutions given the average case $0$-k Clique assumption. 
%\label{thm:kcliqueOWF}
%\end{theorem}
%\begin{proof}
%Computing $f_{kClique}$ takes $n^{k-2}$ time when the instance $I$ has $n$ nodes. 
%
%Let $D_I$ be the uniform distribution over $0$-k clique instances with $0$ solutions. \\
%Let $D_{a,b,c)}$ be the uniform distribution of triples with each value chosen uniformly from $[1,n]$.\\
%Let $D_{w}$ be the distribution from Lemma \ref{lem:kcliquePlant} that generates average case solutions. \\
%Let $D_{in}$ be the distribution made by sampling $I$ from $D_I$, $(a,b,c)$ from $D_{(a,b,c)}$ and $(w_{a,b},\ldots)$ from $D_{w}$.
%
%Let the Adversary have a $\epsilon$ probability of inverting the function when the input to the function is chosen from $D_{in}$.
%
%We will then use the adversary to distinguish a random $0$ solution instance from a $1$ solution instance. If we are given an instance $\hat{I}$ then we give the adversary this instance and ask for an inversion. 
%
%If the instance has $0$ solutions the adversary will be unable to invert (no $(a,b,c)$ triple will form a $0$-triangle). If the instance has $1$ solution, this is by Lemma \ref{lem:kcliquePlant} going to be the same distribution as applying the function to the distribution from $D_{in}$. 
%
%Thus, the Adversary will invert the function and find the $0$-k clique an $\epsilon$ fraction of the time. If the Adversary fails we will guess $0$ solutions, if the Adversary succeeds we will guess $1$ solution. 
%
%Given that  $\hat{I}$ has a $1/2$ chance of being a $0$ input then we guess correctly $1/2+\epsilon$ of the time. 
%
%Thus, the Adversary can not invert the function correctly with probability $\geq 1/3$.
%\end{proof}